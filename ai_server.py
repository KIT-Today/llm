# -*- coding: utf-8 -*-
"""
ğŸ”¥ ë²ˆì•„ì›ƒ ê°ì§€ AI ì„œë²„
=======================

ë°±ì—”ë“œ API ëª…ì„¸ì— ë§ì¶˜ FastAPI ì„œë²„
- POST /analyze : ë¶„ì„ ìš”ì²­ ë°›ê³  ì¦‰ì‹œ 200 OK
- ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ í›„ ì½œë°±ìœ¼ë¡œ ê²°ê³¼ ì „ì†¡

ì‹¤í–‰: uvicorn ai_server:app --reload --port 8001
"""

import os
import asyncio
import httpx
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import re
import random
from datetime import datetime
from typing import List, Dict, Optional, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

# í”„ë¡¬í”„íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
from prompts import (
    PersonaType, 
    PERSONAS,
    PromptBuilder, 
    get_template_feedback,
    get_persona_by_preference,
    CATEGORY_CONTEXT,
)

# ============================================
# ì„¤ì •
# ============================================

class Config:
    # ë°±ì—”ë“œ ì½œë°± URL (ë°°í¬ ì‹œ ë³€ê²½)
    BACKEND_CALLBACK_URL = os.getenv(
        "BACKEND_CALLBACK_URL", 
        "http://127.0.0.1:8000/diaries/analysis-callback"
    )
    
    # ëª¨ë¸ ê²½ë¡œ
    MODEL_DIR = os.getenv("MODEL_DIR", ".")
    
    # ë””ë°”ì´ìŠ¤
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ì†”ë£¨ì…˜ í™œë™ ID ë§¤í•‘ (activities í…Œì´ë¸” ê¸°ì¤€)
    # TODO: ì‹¤ì œ DBì˜ activity_idì— ë§ê²Œ ìˆ˜ì • í•„ìš”
    ACTIVITY_IDS = {
        "ì •ì„œì _ê³ ê°ˆ": [1, 2, 3],      # íœ´ì‹, ëª…ìƒ ë“±
        "ì¢Œì ˆ_ì••ë°•": [4, 5, 6],        # ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë“±
        "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„": [7, 8, 9],   # ì†Œí†µ, ê´€ê³„ íšŒë³µ ë“±
        "ìê¸°ë¹„í•˜": [10, 11, 12],      # ìê¸° ê¸ì •, ì„±ì·¨ê° ë“±
    }


# ============================================
# ì¹´í…Œê³ ë¦¬ ë° í‚¤ì›Œë“œ ì •ì˜
# ============================================

STAGE1_CATEGORIES = {0: "ê¸ì •", 1: "ë¶€ì •"}
STAGE2_CATEGORIES = {0: "ì •ì„œì _ê³ ê°ˆ", 1: "ì¢Œì ˆ_ì••ë°•", 2: "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„", 3: "ìê¸°ë¹„í•˜"}

# MBI ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (ë°±ì—”ë“œ ëª…ì„¸ì— ë§ì¶¤)
MBI_CATEGORY_MAP = {
    "ê¸ì •": "NONE",
    "ì •ì„œì _ê³ ê°ˆ": "EMOTIONAL_EXHAUSTION",
    "ì¢Œì ˆ_ì••ë°•": "FRUSTRATION_PRESSURE",
    "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„": "NEGATIVE_RELATIONSHIP",
    "ìê¸°ë¹„í•˜": "SELF_DEPRECATION"
}

BURNOUT_KEYWORDS = {
    "ê¸ì •": {"keywords": ["ì¢‹ë‹¤", "ì¢‹ì•„", "í–‰ë³µ", "ê¸°ì˜", "ë¿Œë“¯", "ë§Œì¡±", "ê°ì‚¬", "ê³ ë§™", "ë‹¤í–‰", "í™€ê°€ë¶„", "ìƒì¾Œ", "íë§", "í¸ì•ˆ", "ì—¬ìœ ", "ì„±ê³µ", "ë‹¬ì„±", "ì™„ë£Œ", "ëë‚¬", "ì¹­ì°¬", "ì¸ì •", "ë³´ëŒ", "ì¦ê²", "ì‹ ë‚˜", "ì„¤ë ˆ", "ê¸°ëŒ€", "í¬ë§", "ì›ƒ"]},
    "ë¶€ì •": {"keywords": ["í˜ë“¤", "ì§€ì¹˜", "í”¼ê³¤", "ì‹«", "ì§œì¦", "í™”ë‚˜", "ì–µìš¸", "ìŠ¬í”„", "ìš°ìš¸", "ë¶ˆì•ˆ", "ê±±ì •", "ë¬´ì„­", "ë‘ë µ", "ì™¸ë¡­", "ì„œìš´", "ì‹¤ë§", "í›„íšŒ", "ë¯¸ì•ˆ"]},
    "ì •ì„œì _ê³ ê°ˆ": {"keywords": ["ì§€ì¹˜", "í”¼ê³¤", "í˜ë“¤", "ë¬´ê¸°ë ¥", "íƒˆì§„", "ë…¹ì´ˆ", "ë°©ì „", "ì§€ì³", "ì˜ìš•", "ì—ë„ˆì§€", "ê¸°ìš´", "ë¬´ê±°", "ê³µí—ˆ", "í……", "ë¹„ì–´", "ë©”ë§ˆë¥´", "ë²ˆì•„ì›ƒ", "ìš°ìš¸", "ìŠ¬í”„", "ëˆˆë¬¼", "í—ˆë¬´", "ë¬´ì˜ë¯¸", "ì‹«ì–´", "ê·€ì°®"]},
    "ì¢Œì ˆ_ì••ë°•": {"keywords": ["í™”ë‚˜", "í™”ê°€", "ì§œì¦", "ì—´ë°›", "ë¹¡ì¹˜", "ë¶„ë…¸", "ì–µìš¸", "ë¶ˆê³µí‰", "ì••ë°•", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë§ˆê°", "ë‹µë‹µ", "ë¯¸ì¹˜", "í­ë°œ", "í•œê³„", "ëª»ì°¸", "ì™œ", "ë„ëŒ€ì²´", "ì§“ëˆŒ", "ê°ë‹¹", "ë¶€ë‹´", "ì‹¤ì ", "ì•ˆë˜", "ì•ˆí’€"]},
    "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„": {"keywords": ["ë¬´ì‹œ", "ì†Œì™¸", "ë”°ëŒ", "ì™•ë”°", "ë°°ì‹ ", "ë’·ë‹´", "í—˜ë‹´", "ê°ˆë“±", "ì‹¸ìš°", "ë‹¤íˆ¬", "í‹€ì–´", "ì†Œë¬¸", "ì˜¤í•´", "ë¯¿ì—ˆ", "ì‹¤ë§", "ì„œìš´", "í˜¼ì", "ì™¸ë¡œ", "í¸ì—†", "ê±°ì ˆ", "ë¹¼ê³ ", "ì•ˆë¼", "ì •ì¹˜", "ëˆˆì¹˜"]},
    "ìê¸°ë¹„í•˜": {"keywords": ["ëª»í•˜", "ëª»ë‚œ", "ë¶€ì¡±", "ë¬´ëŠ¥", "í•œì‹¬", "ìê²©", "ë¶ˆì•ˆ", "ê±±ì •", "ìì±…", "ì£„ì±…", "ì˜ëª»", "ë‚´íƒ“", "ë¯¸ì•ˆ", "í›„íšŒ", "ì—´ë“±", "ë¹„êµ", "ì™œë‚˜ë§Œ", "ìì‹ ì—†", "ë‘ë µ", "ë¬´ì„­", "ì‹¤íŒ¨", "ë§", "ê°€ì¹˜ì—†", "ì“¸ëª¨ì—†"]},
}


# ============================================
# Pydantic ëª¨ë¸ (Request/Response)
# ============================================

class DiaryHistory(BaseModel):
    diary_id: int
    content: Optional[str] = None
    keywords: Optional[Dict[str, Any]] = None
    created_at: str

class AnalyzeRequest(BaseModel):
    """ë°±ì—”ë“œ â†’ AI ì„œë²„ ë¶„ì„ ìš”ì²­"""
    diary_id: int
    user_id: int
    history: List[DiaryHistory]

class RecommendationItem(BaseModel):
    """ì†”ë£¨ì…˜ ì¶”ì²œ ì•„ì´í…œ"""
    activity_id: int
    ai_message: str

class AnalysisCallback(BaseModel):
    """AI ì„œë²„ â†’ ë°±ì—”ë“œ ì½œë°± ì‘ë‹µ"""
    diary_id: int
    primary_emotion: str          # "ê¸ì •" or "ë¶€ì •"
    primary_score: float          # ì‹ ë¢°ë„ (0~1)
    mbi_category: str             # NONE, EMOTIONAL_EXHAUSTION ë“±
    emotion_probs: Dict[str, float]
    recommendations: List[RecommendationItem]


# ============================================
# ë¶„ë¥˜ ëª¨ë¸ ì •ì˜
# ============================================

class BurnoutClassifier(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=2, dropout=0.5):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)


# ============================================
# AI ë¶„ì„ ì—”ì§„
# ============================================

class BurnoutAnalyzer:
    """ë²ˆì•„ì›ƒ ë¶„ì„ ì—”ì§„ - ì‹±ê¸€í†¤ìœ¼ë¡œ ëª¨ë¸ ìœ ì§€"""
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def initialize(self):
        if self._initialized:
            return
        
        print(f"ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘... (Device: {Config.DEVICE})")
        
        # KURE ì„ë² ë”© ëª¨ë¸
        print("  ğŸ“¥ KURE ë¡œë”©...")
        self.kure = SentenceTransformer("nlpai-lab/KURE-v1", device=Config.DEVICE)
        
        # Stage 1 ëª¨ë¸ (ê¸ì •/ë¶€ì •)
        print("  ğŸ“¥ Stage 1 ëª¨ë¸ ë¡œë”©...")
        s1_path = f"{Config.MODEL_DIR}/stage1_model.pt"
        s1_ckpt = torch.load(s1_path, map_location=Config.DEVICE, weights_only=False)
        self.stage1 = BurnoutClassifier(
            input_dim=s1_ckpt.get('embedding_dim', 1024),
            hidden_dim=s1_ckpt.get('hidden_dim', 256),
            num_classes=2
        ).to(Config.DEVICE)
        self.stage1.load_state_dict(s1_ckpt['model_state_dict'])
        self.stage1.eval()
        
        # Stage 2 ëª¨ë¸ (4ê°œ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬)
        print("  ğŸ“¥ Stage 2 ëª¨ë¸ ë¡œë”©...")
        s2_path = f"{Config.MODEL_DIR}/stage2_model.pt"
        s2_ckpt = torch.load(s2_path, map_location=Config.DEVICE, weights_only=False)
        self.stage2 = BurnoutClassifier(
            input_dim=s2_ckpt.get('embedding_dim', 1024),
            hidden_dim=s2_ckpt.get('hidden_dim', 256),
            num_classes=4
        ).to(Config.DEVICE)
        self.stage2.load_state_dict(s2_ckpt['model_state_dict'])
        self.stage2.eval()
        
        self._initialized = True
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def _tokenize(self, text: str) -> List[str]:
        tokens = re.findall(r"[ê°€-í£]+", text)
        return [t for t in tokens if len(t) >= 2]
    
    def _get_embedding(self, text: str) -> torch.Tensor:
        return self.kure.encode(text, convert_to_tensor=True).unsqueeze(0).to(Config.DEVICE)
    
    def predict_stage1(self, text: str) -> tuple:
        """1ë‹¨ê³„: ê¸ì •/ë¶€ì • ë¶„ë¥˜"""
        with torch.no_grad():
            emb = self._get_embedding(text)
            logits = self.stage1(emb)
            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
            pred = int(np.argmax(probs))
        return pred, probs
    
    def predict_stage2(self, text: str) -> tuple:
        """2ë‹¨ê³„: ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        with torch.no_grad():
            emb = self._get_embedding(text)
            logits = self.stage2(emb)
            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
            pred = int(np.argmax(probs))
        return pred, probs
    
    def extract_keywords(self, text: str, category: str, top_k: int = 3) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if category not in BURNOUT_KEYWORDS:
            return []
        
        keywords = BURNOUT_KEYWORDS[category]["keywords"]
        matched = [kw for kw in keywords if kw in text]
        return matched[:top_k]
    
    def analyze(self, text: str, keywords: Optional[Dict] = None) -> Dict:
        """
        ì „ì²´ ë¶„ì„ ì‹¤í–‰
        - text: ì¼ê¸° ë‚´ìš©
        - keywords: ì‚¬ìš©ìê°€ ì„ íƒí•œ í‚¤ì›Œë“œ (optional)
        """
        # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬
        analysis_text = text or ""
        if keywords:
            # í‚¤ì›Œë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
            keyword_text = " ".join([
                f"{k}: {v}" if isinstance(v, str) else str(v) 
                for k, v in keywords.items()
            ])
            analysis_text = f"{analysis_text} {keyword_text}".strip()
        
        if not analysis_text:
            # ë¶„ì„í•  ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "primary_emotion": "ê¸ì •",
                "primary_score": 0.5,
                "mbi_category": "NONE",
                "emotion_probs": {"ê¸ì •": 0.5, "ë¶€ì •": 0.5},
                "burnout_category": None,
                "burnout_probs": {},
                "keywords": []
            }
        
        # Stage 1: ê¸ì •/ë¶€ì • ë¶„ë¥˜
        s1_pred, s1_probs = self.predict_stage1(analysis_text)
        primary_emotion = STAGE1_CATEGORIES[s1_pred]
        primary_score = float(s1_probs[s1_pred])
        
        result = {
            "primary_emotion": primary_emotion,
            "primary_score": primary_score,
            "emotion_probs": {
                "ê¸ì •": float(s1_probs[0]),
                "ë¶€ì •": float(s1_probs[1])
            },
            "burnout_category": None,
            "burnout_probs": {},
            "keywords": []
        }
        
        # ë¶€ì •ì´ë©´ Stage 2 ì‹¤í–‰
        if s1_pred == 1:  # ë¶€ì •
            s2_pred, s2_probs = self.predict_stage2(analysis_text)
            burnout_category = STAGE2_CATEGORIES[s2_pred]
            
            result["burnout_category"] = burnout_category
            result["burnout_probs"] = {
                STAGE2_CATEGORIES[i]: float(p) for i, p in enumerate(s2_probs)
            }
            result["mbi_category"] = MBI_CATEGORY_MAP[burnout_category]
            result["keywords"] = self.extract_keywords(analysis_text, burnout_category)
        else:
            result["mbi_category"] = "NONE"
        
        return result


# ============================================
# í”¼ë“œë°± ìƒì„±ê¸° (LLM ë˜ëŠ” í…œí”Œë¦¿) - prompts.py ëª¨ë“ˆ í™œìš©
# ============================================

class FeedbackGenerator:
    """
    AI í”¼ë“œë°± ë©”ì‹œì§€ ìƒì„±
    - í…œí”Œë¦¿ ê¸°ë°˜ (ë¹ ë¦„, ê¸°ë³¸) - prompts.pyì˜ FEEDBACK_TEMPLATES ì‚¬ìš©
    - LLM ê¸°ë°˜ (ëŠë¦¼, ê³ í’ˆì§ˆ) - prompts.pyì˜ PromptBuilder ì‚¬ìš©
    
    5ê°€ì§€ í˜ë¥´ì†Œë‚˜ ì§€ì›:
    - WARM_COUNSELOR: ë”°ëœ»í•œ ìƒë‹´ì‚¬
    - PRACTICAL_ADVISOR: ì‹¤ìš©ì  ì¡°ì–¸ì  
    - FRIENDLY_BUDDY: ì¹œê·¼í•œ ì¹œêµ¬
    - CALM_MENTOR: ì°¨ë¶„í•œ ë©˜í† 
    - CHEERFUL_SUPPORTER: ë°ì€ ì‘ì›ë‹¨
    """
    
    def __init__(
        self, 
        use_llm: bool = False, 
        persona_type: PersonaType = PersonaType.WARM_COUNSELOR
    ):
        self.use_llm = use_llm
        self.persona_type = persona_type
        self.prompt_builder = PromptBuilder(persona_type)
        self.generator = None
        self.tokenizer = None
        
        if use_llm:
            self._load_llm()
    
    def set_persona(self, persona_type: PersonaType):
        """í˜ë¥´ì†Œë‚˜ ë³€ê²½"""
        self.persona_type = persona_type
        self.prompt_builder.set_persona(persona_type)
    
    def _load_llm(self):
        """LLM ëª¨ë¸ ë¡œë“œ (ì„ íƒì )"""
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
            
            MODEL_NAME = "beomi/KoAlpaca-Polyglot-5.8B"
            print(f"ğŸ“¥ LLM ë¡œë”© ì¤‘: {MODEL_NAME}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                device_map="auto",
                torch_dtype=torch.float16
            )
            self.generator = pipeline(
                "text-generation",
                model=model,
                tokenizer=self.tokenizer,
                device_map="auto"
            )
            print("âœ… LLM ë¡œë”© ì™„ë£Œ!")
        except Exception as e:
            print(f"âš ï¸ LLM ë¡œë”© ì‹¤íŒ¨, í…œí”Œë¦¿ ëª¨ë“œ ì‚¬ìš©: {e}")
            self.use_llm = False
    
    def generate(
        self, 
        category: str, 
        user_text: str = "", 
        keywords: List[str] = None,
        activity_name: str = "",
        user_preference: dict = None,
    ) -> str:
        """
        í”¼ë“œë°± ìƒì„±
        
        Args:
            category: ê°ì • ì¹´í…Œê³ ë¦¬ (ì •ì„œì _ê³ ê°ˆ, ì¢Œì ˆ_ì••ë°• ë“±)
            user_text: ì‚¬ìš©ì ì¼ê¸° ë‚´ìš©
            keywords: ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡
            activity_name: ì¶”ì²œ í™œë™ ì´ë¦„
            user_preference: ì‚¬ìš©ì ì„¤ë¬¸ ê²°ê³¼ (í˜ë¥´ì†Œë‚˜ ìë™ ì„ íƒìš©)
        """
        # ì‚¬ìš©ì ì„¤ë¬¸ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í˜ë¥´ì†Œë‚˜ ìë™ ì„ íƒ
        if user_preference:
            auto_persona = get_persona_by_preference(user_preference)
            self.set_persona(auto_persona)
        
        if self.use_llm and self.generator:
            return self._generate_llm(category, user_text, keywords, activity_name)
        else:
            return self._generate_template(category, keywords)
    
    def _generate_template(self, category: str, keywords: List[str] = None) -> str:
        """í…œí”Œë¦¿ ê¸°ë°˜ í”¼ë“œë°± - prompts.py í™œìš©"""
        return get_template_feedback(
            persona_type=self.persona_type,
            category=category,
            keywords=keywords
        )
    
    def _generate_llm(
        self, 
        category: str, 
        user_text: str, 
        keywords: List[str],
        activity_name: str
    ) -> str:
        """LLM ê¸°ë°˜ í”¼ë“œë°± - prompts.pyì˜ PromptBuilder í™œìš©"""
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.prompt_builder.build_feedback_prompt(
            category=category,
            user_text=user_text,
            keywords=keywords,
            activity_name=activity_name,
        )
        
        try:
            result = self.generator(
                prompt,
                max_new_tokens=150,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                pad_token_id=self.tokenizer.eos_token_id
            )
            
            generated = result[0]['generated_text']
            response = generated.split("### ì‘ë‹µ")[-1].strip()
            
            # ì¤„ë°”ê¿ˆ ì´í›„ ì˜ë¼ë‚´ê¸°
            if "\n\n" in response:
                response = response.split("\n\n")[0].strip()
            
            # ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì´ìƒí•˜ë©´ í…œí”Œë¦¿ìœ¼ë¡œ í´ë°±
            if len(response) < 10:
                return self._generate_template(category, keywords)
            
            return response
            
        except Exception as e:
            print(f"âš ï¸ LLM ìƒì„± ì‹¤íŒ¨: {e}")
            return self._generate_template(category, keywords)


# ============================================
# ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤
# ============================================

analyzer: Optional[BurnoutAnalyzer] = None
feedback_gen: Optional[FeedbackGenerator] = None


# ============================================
# FastAPI ì•±
# ============================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    global analyzer, feedback_gen
    
    analyzer = BurnoutAnalyzer()
    analyzer.initialize()
    
    # í”¼ë“œë°± ìƒì„±ê¸° (LLM ì‚¬ìš© ì—¬ë¶€ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ)
    use_llm = os.getenv("USE_LLM", "false").lower() == "true"
    default_persona = os.getenv("DEFAULT_PERSONA", "warm_counselor")
    
    # í˜ë¥´ì†Œë‚˜ ë¬¸ìì—´ ë§¤í•‘
    persona_map = {
        "warm_counselor": PersonaType.WARM_COUNSELOR,
        "practical_advisor": PersonaType.PRACTICAL_ADVISOR,
        "friendly_buddy": PersonaType.FRIENDLY_BUDDY,
        "calm_mentor": PersonaType.CALM_MENTOR,
        "cheerful_supporter": PersonaType.CHEERFUL_SUPPORTER,
    }
    persona_type = persona_map.get(default_persona, PersonaType.WARM_COUNSELOR)
    
    feedback_gen = FeedbackGenerator(use_llm=use_llm, persona_type=persona_type)
    
    yield
    
    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    print("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ")


app = FastAPI(
    title="ë²ˆì•„ì›ƒ ê°ì§€ AI ì„œë²„",
    description="í•œêµ­í˜• ë²ˆì•„ì›ƒ ê°ì • ë¶„ì„ ë° í”¼ë“œë°± ìƒì„± API",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================

@app.get("/")
async def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "service": "Burnout Detection AI Server",
        "device": Config.DEVICE,
        "model_loaded": analyzer is not None and analyzer._initialized
    }


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    return {"status": "healthy"}


@app.post("/analyze")
async def analyze_diary(request: AnalyzeRequest, background_tasks: BackgroundTasks):
    """
    ì¼ê¸° ë¶„ì„ ìš”ì²­ (ë°±ì—”ë“œ â†’ AI ì„œë²„)
    
    - ì¦‰ì‹œ 200 OK ë°˜í™˜
    - ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ í›„ ì½œë°±ìœ¼ë¡œ ê²°ê³¼ ì „ì†¡
    """
    # ì…ë ¥ ê²€ì¦
    if not request.history:
        raise HTTPException(status_code=400, detail="historyê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ë¶„ì„ ì‹¤í–‰
    background_tasks.add_task(
        process_analysis,
        diary_id=request.diary_id,
        user_id=request.user_id,
        history=request.history
    )
    
    return {"status": "accepted", "message": "ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."}


async def process_analysis(diary_id: int, user_id: int, history: List[DiaryHistory]):
    """
    ë°±ê·¸ë¼ìš´ë“œ ë¶„ì„ ì²˜ë¦¬
    """
    try:
        print(f"ğŸ” ë¶„ì„ ì‹œì‘: diary_id={diary_id}, user_id={user_id}")
        
        # ìµœì‹  ì¼ê¸° (ì²« ë²ˆì§¸ í•­ëª©)
        latest_diary = history[0]
        content = latest_diary.content or ""
        keywords = latest_diary.keywords or {}
        
        # ë¶„ì„ ì‹¤í–‰
        analysis_result = analyzer.analyze(content, keywords)
        
        # ì†”ë£¨ì…˜ ì¶”ì²œ ìƒì„±
        recommendations = generate_recommendations(
            category=analysis_result.get("burnout_category") or "ê¸ì •",
            user_text=content,
            keywords=analysis_result.get("keywords", [])
        )
        
        # ì½œë°± ë°ì´í„° êµ¬ì„±
        callback_data = AnalysisCallback(
            diary_id=diary_id,
            primary_emotion=analysis_result["primary_emotion"],
            primary_score=round(analysis_result["primary_score"], 4),
            mbi_category=analysis_result["mbi_category"],
            emotion_probs=analysis_result["emotion_probs"],
            recommendations=recommendations
        )
        
        # ë°±ì—”ë“œë¡œ ì½œë°± ì „ì†¡
        await send_callback(callback_data)
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: diary_id={diary_id}, category={analysis_result['mbi_category']}")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: diary_id={diary_id}, error={e}")
        import traceback
        traceback.print_exc()


def generate_recommendations(
    category: str, 
    user_text: str, 
    keywords: List[str]
) -> List[RecommendationItem]:
    """
    ì†”ë£¨ì…˜ ì¶”ì²œ ìƒì„±
    """
    recommendations = []
    
    # ê¸ì •ì´ë©´ ì¶”ì²œ ì—†ìŒ
    if category == "ê¸ì •" or category is None:
        return recommendations
    
    # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ í™œë™ ID ê°€ì ¸ì˜¤ê¸°
    activity_ids = Config.ACTIVITY_IDS.get(category, [1, 2, 3])
    
    # ìµœëŒ€ 3ê°œ ì¶”ì²œ
    selected_ids = random.sample(activity_ids, min(3, len(activity_ids)))
    
    for activity_id in selected_ids:
        # AI ë©”ì‹œì§€ ìƒì„±
        ai_message = feedback_gen.generate(
            category=category,
            user_text=user_text,
            keywords=keywords,
            activity_name=""  # TODO: ì‹¤ì œ í™œë™ëª… ì¡°íšŒ
        )
        
        recommendations.append(RecommendationItem(
            activity_id=activity_id,
            ai_message=ai_message
        ))
    
    return recommendations


async def send_callback(data: AnalysisCallback):
    """
    ë°±ì—”ë“œë¡œ ë¶„ì„ ê²°ê³¼ ì½œë°± ì „ì†¡
    """
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                Config.BACKEND_CALLBACK_URL,
                json=data.model_dump()
            )
            
            if response.status_code == 200:
                print(f"ğŸ“¤ ì½œë°± ì „ì†¡ ì„±ê³µ: diary_id={data.diary_id}")
            else:
                print(f"âš ï¸ ì½œë°± ì „ì†¡ ì‹¤íŒ¨: status={response.status_code}, body={response.text}")
                
    except Exception as e:
        print(f"âŒ ì½œë°± ì „ì†¡ ì—ëŸ¬: {e}")


# ============================================
# í…ŒìŠ¤íŠ¸ìš© ì—”ë“œí¬ì¸íŠ¸
# ============================================

@app.post("/analyze/sync")
async def analyze_sync(request: AnalyzeRequest):
    """
    ë™ê¸° ë¶„ì„ (í…ŒìŠ¤íŠ¸ìš©)
    - ë¶„ì„ ì™„ë£Œ í›„ ê²°ê³¼ ì§ì ‘ ë°˜í™˜
    """
    if not request.history:
        raise HTTPException(status_code=400, detail="historyê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    
    latest_diary = request.history[0]
    content = latest_diary.content or ""
    keywords = latest_diary.keywords or {}
    
    # ë¶„ì„
    analysis_result = analyzer.analyze(content, keywords)
    
    # ì¶”ì²œ ìƒì„±
    recommendations = generate_recommendations(
        category=analysis_result.get("burnout_category") or "ê¸ì •",
        user_text=content,
        keywords=analysis_result.get("keywords", [])
    )
    
    return AnalysisCallback(
        diary_id=request.diary_id,
        primary_emotion=analysis_result["primary_emotion"],
        primary_score=round(analysis_result["primary_score"], 4),
        mbi_category=analysis_result["mbi_category"],
        emotion_probs=analysis_result["emotion_probs"],
        recommendations=recommendations
    )


@app.post("/test/feedback")
async def test_feedback(
    category: str = "ì •ì„œì _ê³ ê°ˆ",
    text: str = "ì˜¤ëŠ˜ ë„ˆë¬´ í˜ë“¤ì—ˆì–´",
    persona: str = "warm_counselor"
):
    """
    í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸
    
    persona ì˜µì…˜:
    - warm_counselor: ë”°ëœ»í•œ ìƒë‹´ì‚¬
    - practical_advisor: ì‹¤ìš©ì  ì¡°ì–¸ì
    - friendly_buddy: ì¹œê·¼í•œ ì¹œêµ¬
    - calm_mentor: ì°¨ë¶„í•œ ë©˜í† 
    - cheerful_supporter: ë°ì€ ì‘ì›ë‹¨
    """
    global feedback_gen
    
    # í˜ë¥´ì†Œë‚˜ ë¬¸ìì—´ ë§¤í•‘
    persona_map = {
        "warm_counselor": PersonaType.WARM_COUNSELOR,
        "practical_advisor": PersonaType.PRACTICAL_ADVISOR,
        "friendly_buddy": PersonaType.FRIENDLY_BUDDY,
        "calm_mentor": PersonaType.CALM_MENTOR,
        "cheerful_supporter": PersonaType.CHEERFUL_SUPPORTER,
    }
    
    persona_type = persona_map.get(persona, PersonaType.WARM_COUNSELOR)
    feedback_gen.set_persona(persona_type)
    
    feedback = feedback_gen.generate(
        category=category,
        user_text=text,
        keywords=["ì§€ì¹¨", "í˜ë“¦"]
    )
    
    persona_info = PERSONAS[persona_type]
    
    return {
        "category": category, 
        "persona": {
            "type": persona,
            "name": persona_info.name,
            "tone": persona_info.tone,
        },
        "feedback": feedback
    }


@app.get("/personas")
async def list_all_personas():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  í˜ë¥´ì†Œë‚˜ ëª©ë¡"""
    from prompts import list_personas
    return {"personas": list_personas()}


# ============================================
# ì‹¤í–‰
# ============================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "ai_server:app",
        host="0.0.0.0",
        port=8001,
        reload=True
    )
