
"""
üîç Î≤àÏïÑÏõÉ 2Îã®Í≥Ñ Î∂ÑÎ•ò ÌåêÎã® Í∑ºÍ±∞ Î∂ÑÏÑù Î™®Îìà
=====================================

3Í∞ÄÏßÄ Î∞©Î≤ïÏúºÎ°ú ÏòàÏ∏° Í∑ºÍ±∞Î•º ÏÑ§Î™Ö:
1. Attention Score - Ïñ¥Îñ§ Îã®Ïñ¥Í∞Ä Ï§ëÏöîÌñàÎäîÏßÄ
2. SHAP - Í∞Å Îã®Ïñ¥Ïùò Í∏∞Ïó¨ÎèÑ ÏàòÏπòÌôî
3. ÌÇ§ÏõåÎìú Îß§Ïπ≠ - ÏÇ¨Ï†Ñ Ï†ïÏùò ÌÇ§ÏõåÎìú Îß§Ïπ≠Î•†
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import re

STAGE1_CATEGORIES = {0: "Í∏çÏ†ï", 1: "Î∂ÄÏ†ï"}
STAGE2_CATEGORIES = {0: "Ï†ïÏÑúÏ†Å_Í≥†Í∞à", 1: "Ï¢åÏ†à_ÏïïÎ∞ï", 2: "Î∂ÄÏ†ïÏ†Å_ÎåÄÏù∏Í¥ÄÍ≥Ñ", 3: "ÏûêÍ∏∞ÎπÑÌïò"}

BURNOUT_KEYWORDS = {
    "Í∏çÏ†ï": {"keywords": ["Ï¢ãÎã§", "Ï¢ãÏïÑ", "ÌñâÎ≥µ", "Í∏∞ÏÅò", "ÎøåÎìØ", "ÎßåÏ°±", "Í∞êÏÇ¨", "Í≥†Îßô", "Îã§Ìñâ", "ÌôÄÍ∞ÄÎ∂Ñ", "ÏÉÅÏæå", "ÌûêÎßÅ", "Ìé∏Ïïà", "Ïó¨Ïú†", "ÏÑ±Í≥µ", "Îã¨ÏÑ±", "ÏôÑÎ£å", "ÎÅùÎÇ¨", "Ïπ≠Ï∞¨", "Ïù∏Ï†ï", "Î≥¥Îûå", "Ï¶êÍ≤Å", "Ïã†ÎÇò", "ÏÑ§Î†à", "Í∏∞ÎåÄ", "Ìù¨Îßù", "ÏõÉ"]},
    "Î∂ÄÏ†ï": {"keywords": ["ÌûòÎì§", "ÏßÄÏπò", "ÌîºÍ≥§", "Ïã´", "ÏßúÏ¶ù", "ÌôîÎÇò", "ÏñµÏö∏", "Ïä¨ÌîÑ", "Ïö∞Ïö∏", "Î∂àÏïà", "Í±±Ï†ï", "Î¨¥ÏÑ≠", "ÎëêÎ†µ", "Ïô∏Î°≠", "ÏÑúÏö¥", "Ïã§Îßù", "ÌõÑÌöå", "ÎØ∏Ïïà"]},
    "Ï†ïÏÑúÏ†Å_Í≥†Í∞à": {"keywords": ["ÏßÄÏπò", "ÌîºÍ≥§", "ÌûòÎì§", "Î¨¥Í∏∞Î†•", "ÌÉàÏßÑ", "ÎÖπÏ¥à", "Î∞©Ï†Ñ", "ÏßÄÏ≥ê", "ÏùòÏöï", "ÏóêÎÑàÏßÄ", "Í∏∞Ïö¥", "Î¨¥Í±∞", "Í≥µÌóà", "ÌÖÖ", "ÎπÑÏñ¥", "Î©îÎßàÎ•¥", "Î≤àÏïÑÏõÉ", "Ïö∞Ïö∏", "Ïä¨ÌîÑ", "ÎààÎ¨º", "ÌóàÎ¨¥", "Î¨¥ÏùòÎØ∏", "Ïã´Ïñ¥", "Í∑ÄÏ∞Æ"]},
    "Ï¢åÏ†à_ÏïïÎ∞ï": {"keywords": ["ÌôîÎÇò", "ÌôîÍ∞Ä", "ÏßúÏ¶ù", "Ïó¥Î∞õ", "Îπ°Ïπò", "Î∂ÑÎÖ∏", "ÏñµÏö∏", "Î∂àÍ≥µÌèâ", "ÏïïÎ∞ï", "Ïä§Ìä∏Î†àÏä§", "ÎßàÍ∞ê", "ÎãµÎãµ", "ÎØ∏Ïπò", "Ìè≠Î∞ú", "ÌïúÍ≥Ñ", "Î™ªÏ∞∏", "Ïôú", "ÎèÑÎåÄÏ≤¥", "ÏßìÎàå", "Í∞êÎãπ", "Î∂ÄÎã¥", "Ïã§Ï†Å", "ÏïàÎêò", "ÏïàÌíÄ"]},
    "Î∂ÄÏ†ïÏ†Å_ÎåÄÏù∏Í¥ÄÍ≥Ñ": {"keywords": ["Î¨¥Ïãú", "ÏÜåÏô∏", "Îî∞Îèå", "ÏôïÎî∞", "Î∞∞Ïã†", "Îí∑Îã¥", "ÌóòÎã¥", "Í∞àÎì±", "Ïã∏Ïö∞", "Îã§Ìà¨", "ÌãÄÏñ¥", "ÏÜåÎ¨∏", "Ïò§Ìï¥", "ÎØøÏóà", "Ïã§Îßù", "ÏÑúÏö¥", "ÌòºÏûê", "Ïô∏Î°ú", "Ìé∏ÏóÜ", "Í±∞Ï†à", "ÎπºÍ≥†", "ÏïàÎÅº", "Ï†ïÏπò", "ÎààÏπò"]},
    "ÏûêÍ∏∞ÎπÑÌïò": {"keywords": ["Î™ªÌïò", "Î™ªÎÇú", "Î∂ÄÏ°±", "Î¨¥Îä•", "ÌïúÏã¨", "ÏûêÍ≤©", "Î∂àÏïà", "Í±±Ï†ï", "ÏûêÏ±Ö", "Ï£ÑÏ±Ö", "ÏûòÎ™ª", "ÎÇ¥ÌÉì", "ÎØ∏Ïïà", "ÌõÑÌöå", "Ïó¥Îì±", "ÎπÑÍµê", "ÏôúÎÇòÎßå", "ÏûêÏã†ÏóÜ", "ÎëêÎ†µ", "Î¨¥ÏÑ≠", "Ïã§Ìå®", "Îßù", "Í∞ÄÏπòÏóÜ", "Ïì∏Î™®ÏóÜ"]},
}


class BurnoutClassifier(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=2, dropout=0.5):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)


class BurnoutExplainer:
    def __init__(self, kure_model, stage1_model, stage2_model, device="cpu"):
        self.kure = kure_model
        self.stage1 = stage1_model
        self.stage2 = stage2_model
        self.device = device

    def _tokenize(self, text):
        tokens = re.findall(r"[Í∞Ä-Ìû£]+", text)
        return [t for t in tokens if len(t) >= 2]

    def _predict_stage1(self, text):
        self.stage1.eval()
        with torch.no_grad():
            emb = self.kure.encode(text, convert_to_tensor=True).unsqueeze(0).to(self.device)
            logits = self.stage1(emb)
            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
            pred = int(np.argmax(probs))
        return pred, probs

    def _predict_stage2(self, text):
        self.stage2.eval()
        with torch.no_grad():
            emb = self.kure.encode(text, convert_to_tensor=True).unsqueeze(0).to(self.device)
            logits = self.stage2(emb)
            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
            pred = int(np.argmax(probs))
        return pred, probs

    def attention_analysis(self, text, stage="stage1", top_k=5):
        tokens = self._tokenize(text)
        if not tokens:
            return []
        if stage == "stage1":
            base_pred, base_probs = self._predict_stage1(text)
        else:
            base_pred, base_probs = self._predict_stage2(text)
        base_conf = base_probs[base_pred]
        importance = {}
        for token in tokens:
            modified = text.replace(token, "", 1)
            if modified.strip():
                if stage == "stage1":
                    _, new_probs = self._predict_stage1(modified)
                else:
                    _, new_probs = self._predict_stage2(modified)
                change = base_conf - new_probs[base_pred]
                importance[token] = max(0, change)
        total = sum(importance.values()) + 1e-10
        importance = {k: v/total for k, v in importance.items()}
        sorted_tokens = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:top_k]
        return [{"token": t, "score": round(s, 4)} for t, s in sorted_tokens]

    def shap_analysis(self, text, stage="stage1", top_k=5):
        tokens = self._tokenize(text)
        if not tokens:
            return []
        if stage == "stage1":
            base_pred, base_probs = self._predict_stage1(text)
        else:
            base_pred, base_probs = self._predict_stage2(text)
        contributions = {}
        for token in tokens:
            modified = text.replace(token, "", 1)
            if modified.strip():
                if stage == "stage1":
                    _, new_probs = self._predict_stage1(modified)
                else:
                    _, new_probs = self._predict_stage2(modified)
                contrib = base_probs[base_pred] - new_probs[base_pred]
                contributions[token] = contrib
        sorted_tokens = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)[:top_k]
        return [{"token": t, "contribution": round(c, 4), "direction": "positive" if c > 0 else "negative"} for t, c in sorted_tokens]

    def keyword_analysis(self, text, categories=None):
        if categories is None:
            categories = list(BURNOUT_KEYWORDS.keys())
        text_lower = text.lower()
        results = {}
        for cat in categories:
            if cat not in BURNOUT_KEYWORDS:
                continue
            keywords = BURNOUT_KEYWORDS[cat]["keywords"]
            matched = [kw for kw in keywords if kw in text_lower]
            match_rate = len(matched) / len(keywords) if keywords else 0
            results[cat] = {"matched_keywords": matched, "match_count": len(matched), "match_rate": round(match_rate, 4)}
        return results

    def explain(self, text, top_k=5):
        s1_pred, s1_probs = self._predict_stage1(text)
        result = {
            "text": text,
            "stage1": {
                "prediction": {"label": s1_pred, "category": STAGE1_CATEGORIES[s1_pred], "confidence": float(s1_probs[s1_pred])},
                "probabilities": {STAGE1_CATEGORIES[i]: float(p) for i, p in enumerate(s1_probs)},
                "attention": self.attention_analysis(text, "stage1", top_k),
                "shap": self.shap_analysis(text, "stage1", top_k),
                "keywords": self.keyword_analysis(text, ["Í∏çÏ†ï", "Î∂ÄÏ†ï"])
            }
        }
        if s1_pred == 1:
            s2_pred, s2_probs = self._predict_stage2(text)
            result["stage2"] = {
                "prediction": {"label": s2_pred, "category": STAGE2_CATEGORIES[s2_pred], "confidence": float(s2_probs[s2_pred])},
                "probabilities": {STAGE2_CATEGORIES[i]: float(p) for i, p in enumerate(s2_probs)},
                "attention": self.attention_analysis(text, "stage2", top_k),
                "shap": self.shap_analysis(text, "stage2", top_k),
                "keywords": self.keyword_analysis(text, list(STAGE2_CATEGORIES.values()))
            }
        return result
