{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”¥ KURE ê¸°ë°˜ í•œêµ­í˜• ë²ˆì•„ì›ƒ ê°ì • ë¶„ë¥˜ ëª¨ë¸\n",
        "## AI Hub ë°ì´í„°ì…‹ í™œìš© ë²„ì „\n",
        "\n",
        "**ë°ì´í„°**: AI Hub ê°ì„±ëŒ€í™” ë§ë­‰ì¹˜ + í•œêµ­ì–´ ê°ì • ì •ë³´ ëŒ€í™” ë°ì´í„°ì…‹\n",
        "\n",
        "**ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ (ë°•ìˆ˜ì • ì™¸, 2018)**:\n",
        "- 0: ì •ì„œì _ê³ ê°ˆ\n",
        "- 1: ì¢Œì ˆ_ì••ë°•\n",
        "- 2: ë¶€ì •ì _ëŒ€ì¸ê´€ê³„\n",
        "- 3: ìê¸°ë¹„í•˜"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. í™˜ê²½ ì„¤ì •"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers transformers torch pandas scikit-learn tqdm openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Google Drive ë§ˆìš´íŠ¸ & ë°ì´í„° ì—…ë¡œë“œ\n",
        "\n",
        "**ë°©ë²• 1**: Google Driveì— ë°ì´í„° ì—…ë¡œë“œ (ê¶Œì¥)\n",
        "- `dataset` í´ë”ë¥¼ Driveì— ì—…ë¡œë“œ\n",
        "\n",
        "**ë°©ë²• 2**: ì§ì ‘ ì—…ë¡œë“œ\n",
        "- JSON íŒŒì¼ ì§ì ‘ ì—…ë¡œë“œ"
      ],
      "metadata": {
        "id": "data-upload-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°©ë²• 1: Google Drive ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë°ì´í„° ê²½ë¡œ ì„¤ì • (ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •)\n",
        "# Driveì— dataset í´ë”ë¥¼ ì—…ë¡œë“œí–ˆë‹¤ë©´:\n",
        "DATA_PATH = \"/content/drive/MyDrive/Burnout/dataset\"  # ë³¸ì¸ ê²½ë¡œë¡œ ìˆ˜ì •!\n",
        "\n",
        "# ê²½ë¡œ í™•ì¸\n",
        "import os\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"âœ“ ë°ì´í„° ê²½ë¡œ í™•ì¸ë¨: {DATA_PATH}\")\n",
        "    print(f\"  í´ë” ë‚´ìš©: {os.listdir(DATA_PATH)}\")\n",
        "else:\n",
        "    print(f\"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {DATA_PATH}\")\n",
        "    print(\"   Driveì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ê²½ë¡œë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\")"
      ],
      "metadata": {
        "id": "drive-mount"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°©ë²• 2: ì§ì ‘ ì—…ë¡œë“œ (Drive ì—†ì´ ì‚¬ìš© ì‹œ)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # JSON íŒŒì¼ ì„ íƒ\n",
        "# DATA_PATH = \"/content\""
      ],
      "metadata": {
        "id": "direct-upload"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ & ê°ì • ë§¤í•‘ ì •ì˜"
      ],
      "metadata": {
        "id": "mapping-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬\n",
        "BURNOUT_CATEGORIES = {\n",
        "    0: \"ì •ì„œì _ê³ ê°ˆ\",\n",
        "    1: \"ì¢Œì ˆ_ì••ë°•\",\n",
        "    2: \"ë¶€ì •ì _ëŒ€ì¸ê´€ê³„\",\n",
        "    3: \"ìê¸°ë¹„í•˜\"\n",
        "}\n",
        "\n",
        "# AI Hub ê°ì • íƒ€ì…(Eì½”ë“œ) â†’ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ ë§¤í•‘\n",
        "# ê°ì„±ëŒ€í™” ë§ë­‰ì¹˜ì˜ emotion type ì½”ë“œ ê¸°ë°˜\n",
        "EMOTION_TYPE_TO_BURNOUT = {\n",
        "    # ì •ì„œì  ê³ ê°ˆ (0) - í”¼ë¡œ, ë¬´ê¸°ë ¥, ìš°ìš¸ ê³„ì—´\n",
        "    \"E20\": 0,  # ìŠ¬í””\n",
        "    \"E22\": 0,  # ë¹„í†µ\n",
        "    \"E24\": 0,  # ìš°ìš¸\n",
        "    \"E26\": 0,  # ë¬´ê¸°ë ¥\n",
        "    \"E40\": 0,  # ìš°ìš¸/ìŠ¬í””\n",
        "    \"E53\": 0,  # ì™¸ë¡œì›€\n",
        "    \"E32\": 0,  # ìŠ¤íŠ¸ë ˆìŠ¤\n",
        "    \n",
        "    # ì¢Œì ˆ/ì••ë°• (1) - ë¶„ë…¸, ì§œì¦, ì–µìš¸ ê³„ì—´\n",
        "    \"E10\": 1,  # ë¶„ë…¸/í™”ë‚¨\n",
        "    \"E15\": 1,  # ì•…ì˜/ì¦ì˜¤\n",
        "    \"E18\": 1,  # í™”ë‚¨/ì§œì¦\n",
        "    \"E19\": 1,  # ì„±ê°€ì‹¬/ê·€ì°®ìŒ\n",
        "    \"E21\": 1,  # ì‹¤ë§\n",
        "    \"E47\": 1,  # ì–µìš¸\n",
        "    \n",
        "    # ë¶€ì •ì  ëŒ€ì¸ê´€ê³„ (2) - ë°°ì‹ , ì†Œì™¸, ê°ˆë“± ê³„ì—´\n",
        "    \"E42\": 2,  # ë°°ì‹ \n",
        "    \"E49\": 2,  # ë²„ë¦¼ë°›ìŒ\n",
        "    \"E51\": 2,  # ì–´ìƒ‰í•¨/ê´€ê³„ê°ˆë“±\n",
        "    \n",
        "    # ìê¸°ë¹„í•˜ (3) - ë¶ˆì•ˆ, ê±±ì •, ìˆ˜ì¹˜ì‹¬ ê³„ì—´\n",
        "    \"E16\": 3,  # ì´ˆì¡°/ì¡°ê¸‰\n",
        "    \"E25\": 3,  # ë§ˆë¹„/ì–¼ì–´ë¶™ìŒ\n",
        "    \"E30\": 3,  # ë¶ˆì•ˆ\n",
        "    \"E35\": 3,  # ë‹¹í˜¹\n",
        "    \"E37\": 3,  # ê±±ì •\n",
        "    \"E50\": 3,  # ë‹¹í™©\n",
        "    \"E54\": 3,  # ì—´ë“±ê°\n",
        "    \"E56\": 3,  # ë¶€ë„ëŸ¬ì›€\n",
        "    \"E57\": 3,  # ìê¸°í˜ì˜¤\n",
        "    \"E58\": 3,  # ìê¸°ë¹„ë‚œ\n",
        "    \n",
        "    # ê¸ì • ê°ì • (ì œì™¸: -1)\n",
        "    \"E62\": -1,  # ì‹ ë¢°\n",
        "    \"E64\": -1,  # ë§Œì¡±\n",
        "    \"E66\": -1,  # í¸ì•ˆ\n",
        "    \"E67\": -1,  # ì•ˆë„\n",
        "    \"E68\": -1,  # ê¸°ì¨\n",
        "    \"E69\": -1,  # ìì‹ ê°\n",
        "}\n",
        "\n",
        "# í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘ (ë°±ì—…)\n",
        "KEYWORD_TO_BURNOUT = {\n",
        "    # ì •ì„œì  ê³ ê°ˆ\n",
        "    \"ì§€ì¹¨\": 0, \"í”¼ê³¤\": 0, \"ë¬´ê¸°ë ¥\": 0, \"ìš°ìš¸\": 0, \"ìŠ¬í””\": 0,\n",
        "    \"ê³µí—ˆ\": 0, \"ì™¸ë¡œ\": 0, \"í˜ë“¤\": 0, \"ì§€ì³\": 0, \"ìŠ¤íŠ¸ë ˆìŠ¤\": 0,\n",
        "    \n",
        "    # ì¢Œì ˆ/ì••ë°•\n",
        "    \"ë¶„ë…¸\": 1, \"í™”ë‚¨\": 1, \"í™”ê°€\": 1, \"ì§œì¦\": 1, \"ì–µìš¸\": 1,\n",
        "    \"ë¶ˆë§Œ\": 1, \"ì‹¤ë§\": 1, \"ì„±ê°€\": 1, \"ê·€ì°®\": 1,\n",
        "    \n",
        "    # ë¶€ì •ì  ëŒ€ì¸ê´€ê³„\n",
        "    \"ë°°ì‹ \": 2, \"ê°ˆë“±\": 2, \"ë¬´ì‹œ\": 2, \"ì†Œì™¸\": 2, \"ì„œìš´\": 2,\n",
        "    \"ë”°ëŒ\": 2, \"ì–´ìƒ‰\": 2,\n",
        "    \n",
        "    # ìê¸°ë¹„í•˜\n",
        "    \"ë¶ˆì•ˆ\": 3, \"ê±±ì •\": 3, \"ì´ˆì¡°\": 3, \"ë‘ë ¤\": 3, \"ë¶€ë„ëŸ¬\": 3,\n",
        "    \"ì°½í”¼\": 3, \"ìì±…\": 3, \"ë‹¹í™©\": 3, \"ì—´ë“±\": 3,\n",
        "}\n",
        "\n",
        "print(\"âœ“ ë§¤í•‘ í…Œì´ë¸” ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "mapping"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. AI Hub ë°ì´í„° ë¡œë”"
      ],
      "metadata": {
        "id": "loader-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"í…ìŠ¤íŠ¸ ì •ì œ\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)  # ë°˜ë³µ ë¬¸ì ì •ê·œí™”\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_burnout_label(emotion_type: str, text: str = \"\") -> int:\n",
        "    \"\"\"\n",
        "    ê°ì • íƒ€ì… ì½”ë“œì™€ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë²ˆì•„ì›ƒ ë ˆì´ë¸” ë°˜í™˜\n",
        "    Returns: 0-3 (ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬) ë˜ëŠ” -1 (ì œì™¸)\n",
        "    \"\"\"\n",
        "    # 1. emotion type ì½”ë“œë¡œ ë§¤í•‘ ì‹œë„\n",
        "    if emotion_type in EMOTION_TYPE_TO_BURNOUT:\n",
        "        return EMOTION_TYPE_TO_BURNOUT[emotion_type]\n",
        "    \n",
        "    # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘\n",
        "    text_lower = text.lower()\n",
        "    for keyword, label in KEYWORD_TO_BURNOUT.items():\n",
        "        if keyword in text_lower:\n",
        "            return label\n",
        "    \n",
        "    return -1  # ë§¤í•‘ ì‹¤íŒ¨\n",
        "\n",
        "\n",
        "def load_emotional_dialogue_json(json_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ê°ì„±ëŒ€í™” ë§ë­‰ì¹˜ JSON ë¡œë“œ\n",
        "    AI Hub í˜•ì‹: [{\"profile\": {..., \"emotion\": {\"type\": \"E18\", ...}}, \"talk\": {\"content\": {...}}}]\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“‚ Loading: {json_path}\")\n",
        "    \n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    records = []\n",
        "    \n",
        "    for item in tqdm(data, desc=\"Parsing\"):\n",
        "        try:\n",
        "            # emotion type ì¶”ì¶œ\n",
        "            emotion_info = item.get('profile', {}).get('emotion', {})\n",
        "            emotion_type = emotion_info.get('type', '')\n",
        "            \n",
        "            # ëŒ€í™” ë‚´ìš© ì¶”ì¶œ\n",
        "            content = item.get('talk', {}).get('content', {})\n",
        "            \n",
        "            # ì‚¬ìš©ì ë°œí™”(HS01, HS02, HS03)ë§Œ ì¶”ì¶œ\n",
        "            for key in ['HS01', 'HS02', 'HS03']:\n",
        "                text = content.get(key, '')\n",
        "                if text and len(text) >= 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°\n",
        "                    text = clean_text(text)\n",
        "                    burnout_label = get_burnout_label(emotion_type, text)\n",
        "                    \n",
        "                    records.append({\n",
        "                        'text': text,\n",
        "                        'emotion_type': emotion_type,\n",
        "                        'burnout_label': burnout_label,\n",
        "                        'source': 'ê°ì„±ëŒ€í™”'\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    df = pd.DataFrame(records)\n",
        "    print(f\"âœ“ Loaded {len(df)} samples\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_continuous_dialogue_xlsx(xlsx_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    í•œêµ­ì–´ ê°ì • ëŒ€í™” ë°ì´í„°ì…‹ (xlsx) ë¡œë“œ\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“‚ Loading: {xlsx_path}\")\n",
        "    \n",
        "    df = pd.read_excel(xlsx_path)\n",
        "    print(f\"  Columns: {df.columns.tolist()}\")\n",
        "    \n",
        "    # ì»¬ëŸ¼ëª…ì— ë”°ë¼ ì¡°ì • í•„ìš”\n",
        "    # ì¼ë°˜ì ì¸ êµ¬ì¡°: ['ë°œí™”', 'ê°ì •', ...] ë˜ëŠ” ['text', 'emotion', ...]\n",
        "    records = []\n",
        "    \n",
        "    # ì‹¤ì œ ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”\n",
        "    text_col = None\n",
        "    emotion_col = None\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if 'ë°œí™”' in col or 'text' in col.lower() or 'sentence' in col.lower():\n",
        "            text_col = col\n",
        "        if 'ê°ì •' in col or 'emotion' in col.lower() or 'label' in col.lower():\n",
        "            emotion_col = col\n",
        "    \n",
        "    if text_col and emotion_col:\n",
        "        for _, row in df.iterrows():\n",
        "            text = str(row[text_col])\n",
        "            emotion = str(row[emotion_col])\n",
        "            \n",
        "            if text and len(text) >= 10:\n",
        "                text = clean_text(text)\n",
        "                burnout_label = get_burnout_label('', text)  # í‚¤ì›Œë“œ ê¸°ë°˜\n",
        "                \n",
        "                records.append({\n",
        "                    'text': text,\n",
        "                    'emotion_type': emotion,\n",
        "                    'burnout_label': burnout_label,\n",
        "                    'source': 'ì—°ì†ëŒ€í™”'\n",
        "                })\n",
        "    \n",
        "    result_df = pd.DataFrame(records)\n",
        "    print(f\"âœ“ Loaded {len(result_df)} samples\")\n",
        "    return result_df\n",
        "\n",
        "\n",
        "print(\"âœ“ ë°ì´í„° ë¡œë” í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "loader"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. ë°ì´í„° ë¡œë“œ & ì „ì²˜ë¦¬"
      ],
      "metadata": {
        "id": "load-data-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°ì„±ëŒ€í™” ë§ë­‰ì¹˜ ë¡œë“œ\n",
        "emotional_json_path = os.path.join(\n",
        "    DATA_PATH,\n",
        "    \"018.ê°ì„±ëŒ€í™”/Training_221115_add/ë¼ë²¨ë§ë°ì´í„°/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json\"\n",
        ")\n",
        "\n",
        "if os.path.exists(emotional_json_path):\n",
        "    df_emotional = load_emotional_dialogue_json(emotional_json_path)\n",
        "else:\n",
        "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {emotional_json_path}\")\n",
        "    print(\"   ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
        "    df_emotional = pd.DataFrame()"
      ],
      "metadata": {
        "id": "load-emotional"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (ì„ íƒ) ì—°ì† ëŒ€í™” ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "continuous_xlsx_path = os.path.join(\n",
        "    DATA_PATH,\n",
        "    \"í•œêµ­ì–´ ê°ì • ì •ë³´ê°€ í¬í•¨ëœ ì—°ì†ì  ëŒ€í™” ë°ì´í„°ì…‹/í•œêµ­ì–´_ì—°ì†ì _ëŒ€í™”_ë°ì´í„°ì…‹.xlsx\"\n",
        ")\n",
        "\n",
        "if os.path.exists(continuous_xlsx_path):\n",
        "    df_continuous = load_continuous_dialogue_xlsx(continuous_xlsx_path)\n",
        "else:\n",
        "    print(f\"íŒŒì¼ ì—†ìŒ (ì„ íƒì‚¬í•­): {continuous_xlsx_path}\")\n",
        "    df_continuous = pd.DataFrame()"
      ],
      "metadata": {
        "id": "load-continuous"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° í•©ì¹˜ê¸° & í•„í„°ë§\n",
        "dfs = [df for df in [df_emotional, df_continuous] if len(df) > 0]\n",
        "\n",
        "if dfs:\n",
        "    df_combined = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"\\nğŸ“Š ì „ì²´ ë°ì´í„°: {len(df_combined)}ê°œ\")\n",
        "    \n",
        "    # ê¸ì • ê°ì •(-1) ì œì™¸\n",
        "    df_filtered = df_combined[df_combined['burnout_label'] != -1].copy()\n",
        "    print(f\"   ë²ˆì•„ì›ƒ ê´€ë ¨ ë°ì´í„°: {len(df_filtered)}ê°œ\")\n",
        "    \n",
        "    # ì¤‘ë³µ ì œê±°\n",
        "    df_filtered = df_filtered.drop_duplicates(subset=['text'])\n",
        "    print(f\"   ì¤‘ë³µ ì œê±° í›„: {len(df_filtered)}ê°œ\")\n",
        "    \n",
        "    # ë¶„í¬ í™•ì¸\n",
        "    print(\"\\nğŸ“ˆ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n",
        "    for label, name in BURNOUT_CATEGORIES.items():\n",
        "        count = len(df_filtered[df_filtered['burnout_label'] == label])\n",
        "        pct = 100 * count / len(df_filtered) if len(df_filtered) > 0 else 0\n",
        "        print(f\"   {label}. {name}: {count}ê°œ ({pct:.1f}%)\")\n",
        "else:\n",
        "    print(\"âŒ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    df_filtered = pd.DataFrame()"
      ],
      "metadata": {
        "id": "combine-filter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
        "if len(df_filtered) > 0:\n",
        "    print(\"\\nğŸ“ ìƒ˜í”Œ ë°ì´í„°:\")\n",
        "    for label in range(4):\n",
        "        sample = df_filtered[df_filtered['burnout_label'] == label].head(2)\n",
        "        print(f\"\\n[{BURNOUT_CATEGORIES[label]}]\")\n",
        "        for _, row in sample.iterrows():\n",
        "            print(f\"  - {row['text'][:50]}...\")"
      ],
      "metadata": {
        "id": "sample-check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train/Val ë¶„í• "
      ],
      "metadata": {
        "id": "split-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if len(df_filtered) > 100:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df_filtered,\n",
        "        test_size=0.1,\n",
        "        stratify=df_filtered['burnout_label'],\n",
        "        random_state=42\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í• :\")\n",
        "    print(f\"   Train: {len(train_df)}ê°œ\")\n",
        "    print(f\"   Val: {len(val_df)}ê°œ\")\n",
        "else:\n",
        "    print(\"ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.\")\n",
        "    train_df = df_filtered\n",
        "    val_df = None"
      ],
      "metadata": {
        "id": "split"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. ëª¨ë¸ ì •ì˜"
      ],
      "metadata": {
        "id": "model-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KUREEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, model_name=\"nlpai-lab/KURE-v1\", freeze_encoder=True, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"ğŸ“¥ Loading KURE model...\")\n",
        "        self.encoder = SentenceTransformer(model_name, device=self.device)\n",
        "        self.embedding_dim = self.encoder.get_sentence_embedding_dimension()\n",
        "        print(f\"âœ“ Embedding dim: {self.embedding_dim}\")\n",
        "        if freeze_encoder:\n",
        "            for param in self.encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, texts):\n",
        "        return self.encoder.encode(texts, convert_to_tensor=True, show_progress_bar=False)\n",
        "\n",
        "\n",
        "class BurnoutClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class KUREBurnoutClassifier(nn.Module):\n",
        "    def __init__(self, model_name=\"nlpai-lab/KURE-v1\", hidden_dim=256,\n",
        "                 num_classes=4, dropout=0.3, freeze_encoder=True, device=None):\n",
        "        super().__init__()\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.embedding_layer = KUREEmbeddingLayer(model_name, freeze_encoder, self.device)\n",
        "        self.classification_head = BurnoutClassificationHead(\n",
        "            self.embedding_layer.embedding_dim, hidden_dim, num_classes, dropout\n",
        "        ).to(self.device)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, texts):\n",
        "        embeddings = self.embedding_layer(texts)\n",
        "        return self.classification_head(embeddings)\n",
        "\n",
        "    def predict(self, texts):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(texts)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            labels = torch.argmax(logits, dim=-1).cpu().tolist()\n",
        "            categories = [BURNOUT_CATEGORIES[l] for l in labels]\n",
        "        return labels, categories, probs\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save({'classification_head': self.classification_head.state_dict()}, path)\n",
        "        print(f\"âœ“ Saved: {path}\")\n",
        "\n",
        "    def load_model(self, path):\n",
        "        ckpt = torch.load(path, map_location=self.device)\n",
        "        self.classification_head.load_state_dict(ckpt['classification_head'])\n",
        "        print(f\"âœ“ Loaded: {path}\")\n",
        "\n",
        "\n",
        "print(\"âœ“ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Dataset & Training"
      ],
      "metadata": {
        "id": "training-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BurnoutDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'text': self.texts[idx], 'label': self.labels[idx]}\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'texts': [item['text'] for item in batch],\n",
        "        'labels': torch.tensor([item['label'] for item in batch], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(model, train_dataset, val_dataset=None, epochs=10, batch_size=16, lr=2e-4):\n",
        "    device = model.device\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn) if val_dataset else None\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.classification_head.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for batch in pbar:\n",
        "            texts, labels = batch['texts'], batch['labels'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(texts)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, pred = torch.max(logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum().item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        history['train_loss'].append(total_loss / len(train_loader))\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        val_acc = 0\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_correct, val_total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    texts, labels = batch['texts'], batch['labels'].to(device)\n",
        "                    logits = model(texts)\n",
        "                    _, pred = torch.max(logits, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (pred == labels).sum().item()\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"  Loss: {history['train_loss'][-1]:.4f} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "print(\"âœ“ Training í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. ëª¨ë¸ í•™ìŠµ ğŸš€"
      ],
      "metadata": {
        "id": "run-training-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ì´ˆê¸°í™”\n",
        "model = KUREBurnoutClassifier(\n",
        "    model_name=\"nlpai-lab/KURE-v1\",\n",
        "    hidden_dim=256,\n",
        "    num_classes=4,\n",
        "    dropout=0.3,\n",
        "    freeze_encoder=True\n",
        ")"
      ],
      "metadata": {
        "id": "init-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset ìƒì„±\n",
        "if len(train_df) > 0:\n",
        "    train_dataset = BurnoutDataset(\n",
        "        train_df['text'].tolist(),\n",
        "        train_df['burnout_label'].tolist()\n",
        "    )\n",
        "    \n",
        "    val_dataset = None\n",
        "    if val_df is not None and len(val_df) > 0:\n",
        "        val_dataset = BurnoutDataset(\n",
        "            val_df['text'].tolist(),\n",
        "            val_df['burnout_label'].tolist()\n",
        "        )\n",
        "    \n",
        "    print(f\"âœ“ Train: {len(train_dataset)}ê°œ\")\n",
        "    print(f\"âœ“ Val: {len(val_dataset) if val_dataset else 0}ê°œ\")\n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "create-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•™ìŠµ ì‹¤í–‰\n",
        "if len(train_df) > 0:\n",
        "    model, history = train_model(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        epochs=10,\n",
        "        batch_size=32,\n",
        "        lr=2e-4\n",
        "    )"
      ],
      "metadata": {
        "id": "run-training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. ëª¨ë¸ ì €ì¥"
      ],
      "metadata": {
        "id": "save-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Driveì— ì €ì¥\n",
        "save_path = \"/content/drive/MyDrive/Burnout/burnout_model.pt\"\n",
        "model.save_model(save_path)"
      ],
      "metadata": {
        "id": "save-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. í…ŒìŠ¤íŠ¸"
      ],
      "metadata": {
        "id": "test-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"ì˜¤ëŠ˜ë„ ì•¼ê·¼ì´ë‹¤. ë„ˆë¬´ ì§€ì¹˜ê³  í˜ë“¤ë‹¤.\",\n",
        "    \"ìƒì‚¬ê°€ ë˜ í™”ë¥¼ ëƒˆë‹¤. ì–µìš¸í•˜ê³  ë¶„í•˜ë‹¤.\",\n",
        "    \"íŒ€ì›ë“¤ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ë‹¤.\",\n",
        "    \"ë‚˜ëŠ” ì™œ ì´ê²ƒë°–ì— ëª»í•˜ëŠ” ê±¸ê¹Œ. ë¶ˆì•ˆí•˜ë‹¤.\",\n",
        "    \"ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.\",\n",
        "]\n",
        "\n",
        "labels, categories, probs = model.predict(test_texts)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”® ì˜ˆì¸¡ ê²°ê³¼\")\n",
        "print(\"=\"*60)\n",
        "for text, cat, prob in zip(test_texts, categories, probs):\n",
        "    print(f\"\\nì…ë ¥: {text}\")\n",
        "    print(f\"ì˜ˆì¸¡: {cat}\")\n",
        "    prob_str = {k: f\"{v:.1%}\" for k, v in zip(BURNOUT_CATEGORIES.values(), prob.cpu().numpy())}\n",
        "    print(f\"í™•ë¥ : {prob_str}\")"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸"
      ],
      "metadata": {
        "id": "custom-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"ì›”ìš”ì¼ ì•„ì¹¨ë¶€í„° ì•¼ê·¼ ì˜ˆê³ ë¥¼ ë°›ì•˜ë‹¤. ë²Œì¨ ì§€ì¹œë‹¤.\"  # @param {type:\"string\"}\n",
        "\n",
        "labels, categories, probs = model.predict([my_text])\n",
        "\n",
        "print(f\"\\nì…ë ¥: {my_text}\")\n",
        "print(f\"\\nğŸ·ï¸ ì˜ˆì¸¡: {categories[0]}\")\n",
        "print(f\"\\nğŸ“Š í™•ë¥ :\")\n",
        "for name, p in zip(BURNOUT_CATEGORIES.values(), probs[0].cpu().numpy()):\n",
        "    bar = \"â–ˆ\" * int(p * 20)\n",
        "    print(f\"  {name}: {bar} {p:.1%}\")"
      ],
      "metadata": {
        "id": "custom-test"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
