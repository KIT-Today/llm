{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”¥ KURE ê¸°ë°˜ í•œêµ­í˜• ë²ˆì•„ì›ƒ 2ë‹¨ê³„ ë¶„ë¥˜ ëª¨ë¸ v2\n",
        "\n",
        "## ğŸ“‹ ê°œìš”\n",
        "- **ëª©ì **: ìŒì„± ì¼ê¸° í…ìŠ¤íŠ¸ì—ì„œ ë²ˆì•„ì›ƒ ê°ì • ë¶„ë¥˜\n",
        "- **ëª¨ë¸**: KURE (Korean Universal Retrieval Embedding) + Classification Head\n",
        "- **ë¶„ë¥˜ ì²´ê³„**: 2ë‹¨ê³„ ë¶„ë¥˜\n",
        "  - Stage 1: ê¸ì • vs ë¶€ì • (2 í´ë˜ìŠ¤)\n",
        "  - Stage 2: ë¶€ì • â†’ 4ê°œ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬\n",
        "\n",
        "## ğŸ†• v2 ê°œì„ ì‚¬í•­\n",
        "1. **Label Smoothing** ì ìš© (ì˜¤ë²„í”¼íŒ… ë°©ì§€)\n",
        "2. **Focal Loss** ì˜µì…˜ (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)\n",
        "3. **Mixup Augmentation** (ë°ì´í„° ì¦ê°•)\n",
        "4. **Warmup + Cosine Annealing** ìŠ¤ì¼€ì¤„ëŸ¬\n",
        "5. **Early Stopping** ê°œì„  (patience + delta)\n",
        "6. **Class Weight** ìë™ ê³„ì‚°\n",
        "\n",
        "## ğŸ“Š ëª©í‘œ ì„±ëŠ¥\n",
        "- Stage 1: 85-90% (ì´ì „: 88%)\n",
        "- Stage 2: 55-60% (ì´ì „: 47.8%)\n",
        "\n",
        "---"
      ],
      "metadata": {"id": "header"}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. í™˜ê²½ ì„¤ì •"
      ],
      "metadata": {"id": "setup-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU í™•ì¸\n",
        "!nvidia-smi\n",
        "\n",
        "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"âœ… Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ],
      "metadata": {"id": "setup"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ìƒìˆ˜ ë° ì¹´í…Œê³ ë¦¬ ì •ì˜"
      ],
      "metadata": {"id": "constants-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¹´í…Œê³ ë¦¬ ì •ì˜\n",
        "STAGE1_CATEGORIES = {0: \"ê¸ì •\", 1: \"ë¶€ì •\"}\n",
        "STAGE2_CATEGORIES = {\n",
        "    0: \"ì •ì„œì _ê³ ê°ˆ\",      # Emotional Exhaustion\n",
        "    1: \"ì¢Œì ˆ_ì••ë°•\",        # Frustration/Pressure\n",
        "    2: \"ë¶€ì •ì _ëŒ€ì¸ê´€ê³„\",   # Negative Interpersonal Relations\n",
        "    3: \"ìê¸°ë¹„í•˜\"          # Self-deprecation\n",
        "}\n",
        "\n",
        "# ë°ì´í„° ê²½ë¡œ (Google Drive ë§ˆìš´íŠ¸ í›„ ìˆ˜ì •)\n",
        "DATA_PATH = \"/content/drive/MyDrive/Burnout\"\n",
        "\n",
        "print(\"âœ… ìƒìˆ˜ ì •ì˜ ì™„ë£Œ\")\n",
        "print(f\"   Stage 1: {list(STAGE1_CATEGORIES.values())}\")\n",
        "print(f\"   Stage 2: {list(STAGE2_CATEGORIES.values())}\")"
      ],
      "metadata": {"id": "constants"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Google Drive ë§ˆìš´íŠ¸ & ë°ì´í„° ë¡œë“œ"
      ],
      "metadata": {"id": "data-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë°ì´í„° íŒŒì¼ í™•ì¸\n",
        "import os\n",
        "print(f\"\\nğŸ“‚ ë°ì´í„° ê²½ë¡œ: {DATA_PATH}\")\n",
        "if os.path.exists(DATA_PATH):\n",
        "    files = os.listdir(DATA_PATH)\n",
        "    print(f\"   íŒŒì¼ ëª©ë¡: {files}\")\n",
        "else:\n",
        "    print(\"   âš ï¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. DATA_PATHë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.\")"
      ],
      "metadata": {"id": "mount"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1 ë°ì´í„° ë¡œë“œ (ê¸ì •/ë¶€ì •)\n",
        "s1_train = pd.read_csv(f\"{DATA_PATH}/stage1_train.csv\")\n",
        "s1_val = pd.read_csv(f\"{DATA_PATH}/stage1_val.csv\")\n",
        "\n",
        "# Stage 2 ë°ì´í„° ë¡œë“œ (4ê°œ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬)\n",
        "s2_train = pd.read_csv(f\"{DATA_PATH}/stage2_train.csv\")\n",
        "s2_val = pd.read_csv(f\"{DATA_PATH}/stage2_val.csv\")\n",
        "\n",
        "print(\"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
        "print(f\"\\n[Stage 1] ê¸ì • vs ë¶€ì •\")\n",
        "print(f\"   Train: {len(s1_train):,}ê°œ\")\n",
        "print(f\"   Val: {len(s1_val):,}ê°œ\")\n",
        "print(f\"   ë¶„í¬:\\n{s1_train['label'].value_counts().to_string()}\")\n",
        "\n",
        "print(f\"\\n[Stage 2] 4ê°œ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬\")\n",
        "print(f\"   Train: {len(s2_train):,}ê°œ\")\n",
        "print(f\"   Val: {len(s2_val):,}ê°œ\")\n",
        "print(f\"   ë¶„í¬:\\n{s2_train['label'].value_counts().to_string()}\")"
      ],
      "metadata": {"id": "load-data"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. KURE ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"
      ],
      "metadata": {"id": "kure-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ”„ KURE ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
        "kure_model = SentenceTransformer('nlpai-lab/KURE-v1')\n",
        "kure_model = kure_model.to(device)\n",
        "\n",
        "# ì„ë² ë”© ì°¨ì› í™•ì¸\n",
        "test_emb = kure_model.encode(\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥\", convert_to_tensor=True)\n",
        "EMBEDDING_DIM = test_emb.shape[0]\n",
        "print(f\"âœ… KURE ë¡œë“œ ì™„ë£Œ\")\n",
        "print(f\"   Embedding Dim: {EMBEDDING_DIM}\")"
      ],
      "metadata": {"id": "load-kure"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. ì„ë² ë”© ìƒì„± (ì‚¬ì „ ê³„ì‚°)\n",
        "\n",
        "KURE ì¸ì½”ë”ëŠ” **Freeze** ìƒíƒœë¡œ ìœ ì§€í•˜ê³ , ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤."
      ],
      "metadata": {"id": "embedding-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(texts, model, batch_size=64, desc=\"Embedding\"):\n",
        "    \"\"\"\n",
        "    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ â†’ KURE ì„ë² ë”© (ì‚¬ì „ ê³„ì‚°)\n",
        "    KURE ì¸ì½”ë”ëŠ” Freeze ìƒíƒœë¡œ, ì„ë² ë”©ë§Œ ì¶”ì¶œ\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=desc):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "            emb = model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
        "            embeddings.append(emb.cpu())\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "print(\"ğŸ”„ Stage 1 ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
        "s1_train_emb = generate_embeddings(s1_train['text'].tolist(), kure_model, desc=\"S1 Train\")\n",
        "s1_val_emb = generate_embeddings(s1_val['text'].tolist(), kure_model, desc=\"S1 Val\")\n",
        "\n",
        "print(\"\\nğŸ”„ Stage 2 ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
        "s2_train_emb = generate_embeddings(s2_train['text'].tolist(), kure_model, desc=\"S2 Train\")\n",
        "s2_val_emb = generate_embeddings(s2_val['text'].tolist(), kure_model, desc=\"S2 Val\")\n",
        "\n",
        "print(f\"\\nâœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ\")\n",
        "print(f\"   S1 Train: {s1_train_emb.shape}\")\n",
        "print(f\"   S2 Train: {s2_train_emb.shape}\")"
      ],
      "metadata": {"id": "generate-embeddings"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜\n",
        "\n",
        "### êµ¬ì¡° ì„¤ëª…\n",
        "```\n",
        "KURE Encoder (Frozen) â†’ 1024-dim embedding\n",
        "                              â†“\n",
        "Classification Head (Trainable):\n",
        "    Linear(1024 â†’ 256) â†’ LayerNorm â†’ ReLU â†’ Dropout(0.5)\n",
        "    Linear(256 â†’ 128)  â†’ LayerNorm â†’ ReLU â†’ Dropout(0.5)\n",
        "    Linear(128 â†’ num_classes)\n",
        "```\n",
        "\n",
        "### Freeze/Fine-tuning ì„¤ëª…\n",
        "- **KURE Encoder**: ì™„ì „íˆ **Freeze** (ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ìœ ì§€)\n",
        "- **Classification Head**: **Fine-tuning** (ìƒˆë¡œ í•™ìŠµ)"
      ],
      "metadata": {"id": "model-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "class BurnoutClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    ë²ˆì•„ì›ƒ ë¶„ë¥˜ í—¤ë“œ\n",
        "    \n",
        "    ì•„í‚¤í…ì²˜:\n",
        "    - 3ì¸µ MLP with LayerNorm\n",
        "    - Dropout 0.5 (ì˜¤ë²„í”¼íŒ… ë°©ì§€)\n",
        "    - ì…ë ¥: KURE ì„ë² ë”© (1024-dim)\n",
        "    - ì¶œë ¥: num_classes ë¡œì§“\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=2, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            # Layer 1: 1024 â†’ 256\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            # Layer 2: 256 â†’ 128\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.LayerNorm(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            \n",
        "            # Output Layer: 128 â†’ num_classes\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class EmbeddingDataset(Dataset):\n",
        "    \"\"\"ì‚¬ì „ ê³„ì‚°ëœ ì„ë² ë”© ë°ì´í„°ì…‹\"\"\"\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "print(\"âœ… ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")\n",
        "print(\"\\nğŸ“ ëª¨ë¸ êµ¬ì¡°:\")\n",
        "print(\"   KURE Encoder: Frozen (1024-dim output)\")\n",
        "print(\"   Classification Head: Trainable\")\n",
        "print(\"     - Linear(1024â†’256) + LayerNorm + ReLU + Dropout(0.5)\")\n",
        "print(\"     - Linear(256â†’128) + LayerNorm + ReLU + Dropout(0.5)\")\n",
        "print(\"     - Linear(128â†’num_classes)\")"
      ],
      "metadata": {"id": "model-definition"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. ğŸ†• ê°œì„ ëœ Loss Functions\n",
        "\n",
        "### ì ìš©ëœ ê¸°ë²•:\n",
        "1. **Label Smoothing**: ì˜¤ë²„í”¼íŒ… ë°©ì§€, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
        "2. **Focal Loss**: ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ì§‘ì¤‘ (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)\n",
        "3. **Class Weight**: ë°ì´í„° ë¶ˆê· í˜• ë³´ì •"
      ],
      "metadata": {"id": "loss-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss: ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ì§‘ì¤‘\n",
        "    FL(p_t) = -Î±_t * (1 - p_t)^Î³ * log(p_t)\n",
        "    \n",
        "    - gamma: focusing parameter (ë†’ì„ìˆ˜ë¡ ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘)\n",
        "    - alpha: class weight (ë¶ˆê· í˜• ë³´ì •)\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=None, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.label_smoothing = label_smoothing\n",
        "    \n",
        "    def forward(self, inputs, targets):\n",
        "        # Label Smoothing ì ìš©\n",
        "        num_classes = inputs.size(-1)\n",
        "        smooth_targets = torch.zeros_like(inputs).scatter_(\n",
        "            1, targets.unsqueeze(1), 1.0\n",
        "        )\n",
        "        smooth_targets = smooth_targets * (1 - self.label_smoothing) + \\\n",
        "                        self.label_smoothing / num_classes\n",
        "        \n",
        "        # Focal Loss ê³„ì‚°\n",
        "        log_probs = F.log_softmax(inputs, dim=-1)\n",
        "        probs = torch.exp(log_probs)\n",
        "        \n",
        "        focal_weight = (1 - probs) ** self.gamma\n",
        "        loss = -focal_weight * smooth_targets * log_probs\n",
        "        \n",
        "        # Class weight ì ìš©\n",
        "        if self.alpha is not None:\n",
        "            alpha_weight = self.alpha[targets].unsqueeze(1)\n",
        "            loss = loss * alpha_weight\n",
        "        \n",
        "        return loss.sum(dim=-1).mean()\n",
        "\n",
        "\n",
        "def compute_class_weights(labels, num_classes):\n",
        "    \"\"\"\n",
        "    í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ì—­ë¹ˆë„ ê¸°ë°˜)\n",
        "    ì ì€ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
        "    \"\"\"\n",
        "    counts = np.bincount(labels, minlength=num_classes)\n",
        "    weights = 1.0 / (counts + 1e-6)\n",
        "    weights = weights / weights.sum() * num_classes  # ì •ê·œí™”\n",
        "    return torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "\n",
        "print(\"âœ… Loss Functions ì •ì˜ ì™„ë£Œ\")\n",
        "print(\"   - FocalLoss (gamma=2.0, label_smoothing=0.1)\")\n",
        "print(\"   - Class Weight ìë™ ê³„ì‚°\")"
      ],
      "metadata": {"id": "loss-functions"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. ğŸ†• ê°œì„ ëœ í•™ìŠµ í•¨ìˆ˜\n",
        "\n",
        "### ì ìš©ëœ ê°œì„ ì‚¬í•­:\n",
        "1. **Warmup + Cosine Annealing** ìŠ¤ì¼€ì¤„ëŸ¬\n",
        "2. **Early Stopping** (patience=7, min_delta=0.001)\n",
        "3. **Gradient Clipping** (max_norm=1.0)\n",
        "4. **Best Model Checkpoint** ì €ì¥"
      ],
      "metadata": {"id": "training-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    train_emb, train_labels,\n",
        "    val_emb, val_labels,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-4,\n",
        "    patience=7,\n",
        "    min_delta=0.001,\n",
        "    use_focal_loss=True,\n",
        "    label_smoothing=0.1,\n",
        "    warmup_epochs=5,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    ê°œì„ ëœ í•™ìŠµ í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        use_focal_loss: Focal Loss ì‚¬ìš© ì—¬ë¶€\n",
        "        label_smoothing: Label Smoothing ê°•ë„\n",
        "        warmup_epochs: Learning Rate Warmup ê¸°ê°„\n",
        "    \"\"\"\n",
        "    # ë°ì´í„° ë¡œë”\n",
        "    train_dataset = EmbeddingDataset(train_emb, train_labels)\n",
        "    val_dataset = EmbeddingDataset(val_emb, val_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # Class Weight ê³„ì‚°\n",
        "    num_classes = len(np.unique(train_labels))\n",
        "    class_weights = compute_class_weights(train_labels, num_classes).to(device)\n",
        "    print(f\"   Class Weights: {class_weights.cpu().numpy().round(2)}\")\n",
        "    \n",
        "    # Loss Function\n",
        "    if use_focal_loss:\n",
        "        criterion = FocalLoss(gamma=2.0, alpha=class_weights, label_smoothing=label_smoothing)\n",
        "        print(f\"   Loss: FocalLoss (Î³=2.0, smoothing={label_smoothing})\")\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n",
        "        print(f\"   Loss: CrossEntropy (smoothing={label_smoothing})\")\n",
        "    \n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    \n",
        "    # Scheduler: Warmup + Cosine Annealing\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < warmup_epochs:\n",
        "            return (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)\n",
        "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
        "    \n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "    \n",
        "    # í•™ìŠµ ê¸°ë¡\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
        "        'lr': []\n",
        "    }\n",
        "    \n",
        "    # Early Stopping\n",
        "    best_val_f1 = 0\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸš€ í•™ìŠµ ì‹œì‘ (epochs={epochs}, batch={batch_size}, lr={lr})\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # ========== Training ==========\n",
        "        model.train()\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "        \n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
        "        for emb, labels in pbar:\n",
        "            emb, labels = emb.to(device), labels.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            logits = model(emb)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient Clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            _, pred = torch.max(logits, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (pred == labels).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        train_acc = 100 * train_correct / train_total\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        \n",
        "        # ========== Validation ==========\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for emb, labels in val_loader:\n",
        "                emb, labels = emb.to(device), labels.to(device)\n",
        "                logits = model(emb)\n",
        "                loss = criterion(logits, labels)\n",
        "                \n",
        "                val_loss += loss.item()\n",
        "                _, pred = torch.max(logits, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (pred == labels).sum().item()\n",
        "                \n",
        "                all_preds.extend(pred.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "        \n",
        "        val_acc = 100 * val_correct / val_total\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        \n",
        "        # ê¸°ë¡\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_f1'].append(val_f1)\n",
        "        history['lr'].append(current_lr)\n",
        "        \n",
        "        # ì¶œë ¥\n",
        "        print(f\"Epoch {epoch+1:2d} | Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | Val: {val_acc:.1f}% | \"\n",
        "              f\"F1: {val_f1:.4f} | LR: {current_lr:.6f}\")\n",
        "        \n",
        "        # Best Model & Early Stopping\n",
        "        if val_f1 > best_val_f1 + min_delta:\n",
        "            best_val_f1 = val_f1\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "            print(f\"         âœ… Best F1: {best_val_f1:.4f} (saved)\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nâš ï¸ Early Stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "    \n",
        "    # Best Model ë³µì›\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"\\nâœ… Best Model ë³µì› (F1: {best_val_f1:.4f})\")\n",
        "    \n",
        "    return model, history, {'best_f1': best_val_f1, 'best_acc': max(history['val_acc'])}\n",
        "\n",
        "\n",
        "print(\"âœ… í•™ìŠµ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {"id": "training-function"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Stage 1 í•™ìŠµ: ê¸ì • vs ë¶€ì •"
      ],
      "metadata": {"id": "stage1-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š STAGE 1: ê¸ì • vs ë¶€ì • ë¶„ë¥˜\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "S1_CONFIG = {\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'epochs': 50,\n",
        "    'batch_size': 64,\n",
        "    'lr': 5e-4,\n",
        "    'weight_decay': 1e-4,\n",
        "    'patience': 7,\n",
        "    'use_focal_loss': False,  # Stage 1ì€ ê· í˜• ë°ì´í„°ë¼ CE ì‚¬ìš©\n",
        "    'label_smoothing': 0.1,\n",
        "    'warmup_epochs': 3\n",
        "}\n",
        "\n",
        "print(f\"\\nâš™ï¸ ì„¤ì •:\")\n",
        "for k, v in S1_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "stage1_model = BurnoutClassifier(\n",
        "    input_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=S1_CONFIG['hidden_dim'],\n",
        "    num_classes=2,\n",
        "    dropout=S1_CONFIG['dropout']\n",
        ").to(device)\n",
        "\n",
        "# í•™ìŠµ\n",
        "stage1_model, s1_history, s1_best = train_model(\n",
        "    model=stage1_model,\n",
        "    train_emb=s1_train_emb,\n",
        "    train_labels=s1_train['label'].values,\n",
        "    val_emb=s1_val_emb,\n",
        "    val_labels=s1_val['label'].values,\n",
        "    epochs=S1_CONFIG['epochs'],\n",
        "    batch_size=S1_CONFIG['batch_size'],\n",
        "    lr=S1_CONFIG['lr'],\n",
        "    weight_decay=S1_CONFIG['weight_decay'],\n",
        "    patience=S1_CONFIG['patience'],\n",
        "    use_focal_loss=S1_CONFIG['use_focal_loss'],\n",
        "    label_smoothing=S1_CONFIG['label_smoothing'],\n",
        "    warmup_epochs=S1_CONFIG['warmup_epochs'],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ Stage 1 ìµœì¢… ê²°ê³¼:\")\n",
        "print(f\"   Best Val Acc: {s1_best['best_acc']:.2f}%\")\n",
        "print(f\"   Best Val F1: {s1_best['best_f1']:.4f}\")"
      ],
      "metadata": {"id": "stage1-train"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Stage 2 í•™ìŠµ: 4ê°œ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬"
      ],
      "metadata": {"id": "stage2-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"ğŸ“Š STAGE 2: 4ê°œ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# í•˜ì´í¼íŒŒë¼ë¯¸í„° (Stage 2ëŠ” ì–´ë ¤ìš°ë¯€ë¡œ ì¡°ì •)\n",
        "S2_CONFIG = {\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'epochs': 80,\n",
        "    'batch_size': 64,\n",
        "    'lr': 3e-4,           # ë” ë‚®ì€ LR\n",
        "    'weight_decay': 1e-4,\n",
        "    'patience': 10,       # ë” ê¸´ patience\n",
        "    'use_focal_loss': True,  # Focal Loss ì‚¬ìš© (ë¶ˆê· í˜• ë°ì´í„°)\n",
        "    'label_smoothing': 0.15, # ë” ê°•í•œ smoothing\n",
        "    'warmup_epochs': 5\n",
        "}\n",
        "\n",
        "print(f\"\\nâš™ï¸ ì„¤ì •:\")\n",
        "for k, v in S2_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n",
        "\n",
        "# ëª¨ë¸ ìƒì„±\n",
        "stage2_model = BurnoutClassifier(\n",
        "    input_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=S2_CONFIG['hidden_dim'],\n",
        "    num_classes=4,\n",
        "    dropout=S2_CONFIG['dropout']\n",
        ").to(device)\n",
        "\n",
        "# í•™ìŠµ\n",
        "stage2_model, s2_history, s2_best = train_model(\n",
        "    model=stage2_model,\n",
        "    train_emb=s2_train_emb,\n",
        "    train_labels=s2_train['label'].values,\n",
        "    val_emb=s2_val_emb,\n",
        "    val_labels=s2_val['label'].values,\n",
        "    epochs=S2_CONFIG['epochs'],\n",
        "    batch_size=S2_CONFIG['batch_size'],\n",
        "    lr=S2_CONFIG['lr'],\n",
        "    weight_decay=S2_CONFIG['weight_decay'],\n",
        "    patience=S2_CONFIG['patience'],\n",
        "    use_focal_loss=S2_CONFIG['use_focal_loss'],\n",
        "    label_smoothing=S2_CONFIG['label_smoothing'],\n",
        "    warmup_epochs=S2_CONFIG['warmup_epochs'],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ Stage 2 ìµœì¢… ê²°ê³¼:\")\n",
        "print(f\"   Best Val Acc: {s2_best['best_acc']:.2f}%\")\n",
        "print(f\"   Best Val F1: {s2_best['best_f1']:.4f}\")"
      ],
      "metadata": {"id": "stage2-train"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. ê²°ê³¼ ì‹œê°í™”"
      ],
      "metadata": {"id": "viz-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Stage 1\n",
        "axes[0, 0].plot(s1_history['train_loss'], label='Train')\n",
        "axes[0, 0].plot(s1_history['val_loss'], label='Val')\n",
        "axes[0, 0].set_title('Stage 1: Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].plot(s1_history['train_acc'], label='Train')\n",
        "axes[0, 1].plot(s1_history['val_acc'], label='Val')\n",
        "axes[0, 1].set_title(f'Stage 1: Accuracy (Best: {s1_best[\"best_acc\"]:.1f}%)')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "axes[0, 2].plot(s1_history['val_f1'])\n",
        "axes[0, 2].set_title(f'Stage 1: F1 Score (Best: {s1_best[\"best_f1\"]:.4f})')\n",
        "axes[0, 2].set_xlabel('Epoch')\n",
        "\n",
        "# Stage 2\n",
        "axes[1, 0].plot(s2_history['train_loss'], label='Train')\n",
        "axes[1, 0].plot(s2_history['val_loss'], label='Val')\n",
        "axes[1, 0].set_title('Stage 2: Loss')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].plot(s2_history['train_acc'], label='Train')\n",
        "axes[1, 1].plot(s2_history['val_acc'], label='Val')\n",
        "axes[1, 1].set_title(f'Stage 2: Accuracy (Best: {s2_best[\"best_acc\"]:.1f}%)')\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "axes[1, 2].plot(s2_history['val_f1'])\n",
        "axes[1, 2].set_title(f'Stage 2: F1 Score (Best: {s2_best[\"best_f1\"]:.4f})')\n",
        "axes[1, 2].set_xlabel('Epoch')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{DATA_PATH}/training_curves_v2.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nâœ… ê·¸ë˜í”„ ì €ì¥: {DATA_PATH}/training_curves_v2.png\")"
      ],
      "metadata": {"id": "visualization"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. ìƒì„¸ í‰ê°€ (Classification Report & Confusion Matrix)"
      ],
      "metadata": {"id": "eval-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, embeddings, labels, categories, stage_name, device='cuda'):\n",
        "    \"\"\"ëª¨ë¸ ìƒì„¸ í‰ê°€\"\"\"\n",
        "    model.eval()\n",
        "    dataset = EmbeddingDataset(embeddings, labels)\n",
        "    loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
        "    \n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for emb, lab in loader:\n",
        "            emb = emb.to(device)\n",
        "            logits = model(emb)\n",
        "            _, pred = torch.max(logits, 1)\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(lab.numpy())\n",
        "    \n",
        "    # Classification Report\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸ“Š {stage_name} ìƒì„¸ í‰ê°€\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(classification_report(\n",
        "        all_labels, all_preds,\n",
        "        target_names=list(categories.values()),\n",
        "        digits=4\n",
        "    ))\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(pd.DataFrame(\n",
        "        cm,\n",
        "        index=[f\"ì‹¤ì œ_{v}\" for v in categories.values()],\n",
        "        columns=[f\"ì˜ˆì¸¡_{v}\" for v in categories.values()]\n",
        "    ))\n",
        "    \n",
        "    return all_preds, all_labels, cm\n",
        "\n",
        "\n",
        "# Stage 1 í‰ê°€\n",
        "s1_preds, s1_labels, s1_cm = evaluate_model(\n",
        "    stage1_model, s1_val_emb, s1_val['label'].values,\n",
        "    STAGE1_CATEGORIES, \"Stage 1 (ê¸ì •/ë¶€ì •)\", device\n",
        ")\n",
        "\n",
        "# Stage 2 í‰ê°€\n",
        "s2_preds, s2_labels, s2_cm = evaluate_model(\n",
        "    stage2_model, s2_val_emb, s2_val['label'].values,\n",
        "    STAGE2_CATEGORIES, \"Stage 2 (ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬)\", device\n",
        ")"
      ],
      "metadata": {"id": "evaluation"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. ëª¨ë¸ ì €ì¥"
      ],
      "metadata": {"id": "save-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1 ì €ì¥\n",
        "s1_save_path = f\"{DATA_PATH}/stage1_model_v2.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': stage1_model.state_dict(),\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'hidden_dim': S1_CONFIG['hidden_dim'],\n",
        "    'num_classes': 2,\n",
        "    'dropout': S1_CONFIG['dropout'],\n",
        "    'categories': STAGE1_CATEGORIES,\n",
        "    'config': S1_CONFIG,\n",
        "    'best_metrics': s1_best,\n",
        "    'history': s1_history\n",
        "}, s1_save_path)\n",
        "print(f\"âœ… Stage 1 ì €ì¥: {s1_save_path}\")\n",
        "\n",
        "# Stage 2 ì €ì¥\n",
        "s2_save_path = f\"{DATA_PATH}/stage2_model_v2.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': stage2_model.state_dict(),\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'hidden_dim': S2_CONFIG['hidden_dim'],\n",
        "    'num_classes': 4,\n",
        "    'dropout': S2_CONFIG['dropout'],\n",
        "    'categories': STAGE2_CATEGORIES,\n",
        "    'config': S2_CONFIG,\n",
        "    'best_metrics': s2_best,\n",
        "    'history': s2_history\n",
        "}, s2_save_path)\n",
        "print(f\"âœ… Stage 2 ì €ì¥: {s2_save_path}\")"
      ],
      "metadata": {"id": "save-models"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. í†µí•© ì¶”ë¡  í…ŒìŠ¤íŠ¸"
      ],
      "metadata": {"id": "inference-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_2stage(text, kure, stage1, stage2, device='cuda'):\n",
        "    \"\"\"\n",
        "    2ë‹¨ê³„ ë¶„ë¥˜ ì¶”ë¡ \n",
        "    \n",
        "    1. Stage 1: ê¸ì •/ë¶€ì • ë¶„ë¥˜\n",
        "    2. Stage 2: ë¶€ì •ì´ë©´ â†’ 4ê°œ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜\n",
        "    \"\"\"\n",
        "    stage1.eval()\n",
        "    stage2.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # KURE ì„ë² ë”©\n",
        "        emb = kure.encode(text, convert_to_tensor=True).unsqueeze(0).to(device)\n",
        "        \n",
        "        # Stage 1\n",
        "        s1_logits = stage1(emb)\n",
        "        s1_probs = F.softmax(s1_logits, dim=-1)[0]\n",
        "        s1_pred = torch.argmax(s1_logits, dim=-1).item()\n",
        "        \n",
        "        result = {\n",
        "            'text': text,\n",
        "            'stage1': {\n",
        "                'label': s1_pred,\n",
        "                'category': STAGE1_CATEGORIES[s1_pred],\n",
        "                'confidence': s1_probs[s1_pred].item(),\n",
        "                'probs': {STAGE1_CATEGORIES[i]: f\"{p.item():.1%}\" for i, p in enumerate(s1_probs)}\n",
        "            },\n",
        "            'stage2': None\n",
        "        }\n",
        "        \n",
        "        # Stage 2 (ë¶€ì •ì¸ ê²½ìš°ë§Œ)\n",
        "        if s1_pred == 1:\n",
        "            s2_logits = stage2(emb)\n",
        "            s2_probs = F.softmax(s2_logits, dim=-1)[0]\n",
        "            s2_pred = torch.argmax(s2_logits, dim=-1).item()\n",
        "            \n",
        "            result['stage2'] = {\n",
        "                'label': s2_pred,\n",
        "                'category': STAGE2_CATEGORIES[s2_pred],\n",
        "                'confidence': s2_probs[s2_pred].item(),\n",
        "                'probs': {STAGE2_CATEGORIES[i]: f\"{p.item():.1%}\" for i, p in enumerate(s2_probs)}\n",
        "            }\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "def print_result(result):\n",
        "    \"\"\"ê²°ê³¼ ì¶œë ¥\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"ğŸ“ ì…ë ¥: {result['text']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    s1 = result['stage1']\n",
        "    print(f\"[Stage 1] {s1['category']} ({s1['confidence']:.1%})\")\n",
        "    print(f\"   í™•ë¥ : {s1['probs']}\")\n",
        "    \n",
        "    if result['stage2']:\n",
        "        s2 = result['stage2']\n",
        "        print(f\"[Stage 2] {s2['category']} ({s2['confidence']:.1%})\")\n",
        "        print(f\"   í™•ë¥ : {s2['probs']}\")\n",
        "        print(f\"\\nâš ï¸ ê²°ê³¼: {s2['category']} (ë²ˆì•„ì›ƒ ì§•í›„ ê°ì§€)\")\n",
        "    else:\n",
        "        print(f\"\\nâœ… ê²°ê³¼: ê¸ì • - ê´œì°®ì•„ ë³´ì—¬ìš”!\")\n",
        "\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "test_texts = [\n",
        "    \"ì˜¤ëŠ˜ í”„ë¡œì íŠ¸ê°€ ì˜ ëë‚˜ì„œ ê¸°ë¶„ì´ ì¢‹ë‹¤!\",\n",
        "    \"ì˜¤ëŠ˜ë„ ì•¼ê·¼ì´ë‹¤. ë„ˆë¬´ ì§€ì¹˜ê³  í˜ë“¤ë‹¤.\",\n",
        "    \"ìƒì‚¬ê°€ ë˜ í™”ë¥¼ ëƒˆë‹¤. ì–µìš¸í•˜ê³  ë¶„í•˜ë‹¤.\",\n",
        "    \"íŒ€ì›ë“¤ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ë‹¤.\",\n",
        "    \"ë‚˜ëŠ” ì™œ ì´ê²ƒë°–ì— ëª»í•˜ëŠ” ê±¸ê¹Œ. ë¶ˆì•ˆí•˜ë‹¤.\",\n",
        "    \"ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.\",\n",
        "    \"ë§¤ì¼ ê°™ì€ ì¼ì˜ ë°˜ë³µì´ë‹¤. ì˜ë¯¸ë¥¼ ëª¨ë¥´ê² ë‹¤.\",\n",
        "    \"íšŒì˜ì—ì„œ ë‚´ ì˜ê²¬ì´ ë¬´ì‹œë‹¹í–ˆë‹¤. ì„œìš´í•˜ë‹¤.\"\n",
        "]\n",
        "\n",
        "print(\"\\nğŸ§ª 2ë‹¨ê³„ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸\")\n",
        "for text in test_texts:\n",
        "    result = predict_2stage(text, kure_model, stage1_model, stage2_model, device)\n",
        "    print_result(result)"
      ],
      "metadata": {"id": "inference-test"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. ğŸ“‹ ìµœì¢… ìš”ì•½ (êµìˆ˜ë‹˜ ë³´ê³ ìš©)"
      ],
      "metadata": {"id": "summary-header"}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ğŸ“‹ ëª¨ë¸ í•™ìŠµ ìµœì¢… ìš”ì•½\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nğŸ”§ ëª¨ë¸ êµ¬ì¡°\")\n",
        "print(\"-\"*70)\n",
        "print(\"1. KURE Encoder (nlpai-lab/KURE-v1)\")\n",
        "print(\"   - ìƒíƒœ: FROZEN (ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ìœ ì§€)\")\n",
        "print(\"   - ì¶œë ¥: 1024ì°¨ì› ì„ë² ë”©\")\n",
        "print(\"   - ì—­í• : í•œêµ­ì–´ í…ìŠ¤íŠ¸ â†’ ì˜ë¯¸ ë²¡í„° ë³€í™˜\")\n",
        "print(\"\\n2. Classification Head (NEW, Trainable)\")\n",
        "print(\"   - Layer 1: Linear(1024â†’256) + LayerNorm + ReLU + Dropout(0.5)\")\n",
        "print(\"   - Layer 2: Linear(256â†’128) + LayerNorm + ReLU + Dropout(0.5)\")\n",
        "print(\"   - Output: Linear(128â†’num_classes)\")\n",
        "\n",
        "print(\"\\nğŸ“Š ë°ì´í„°ì…‹\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Stage 1 (ê¸ì •/ë¶€ì •): Train {len(s1_train):,}ê°œ, Val {len(s1_val):,}ê°œ\")\n",
        "print(f\"Stage 2 (4ê°œ ì¹´í…Œê³ ë¦¬): Train {len(s2_train):,}ê°œ, Val {len(s2_val):,}ê°œ\")\n",
        "print(\"ì¶œì²˜: AI Hub ê°ì„±ëŒ€í™” ë§ë­‰ì¹˜ (27ë§Œ ë¬¸ì¥) + ì—°ì†ì  ëŒ€í™” ë°ì´í„°ì…‹ (5.5ë§Œ ë¬¸ì¥)\")\n",
        "\n",
        "print(\"\\nğŸ¯ ì„±ëŠ¥ ê²°ê³¼\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Stage 1 (ê¸ì •/ë¶€ì •):     Acc = {s1_best['best_acc']:.2f}%, F1 = {s1_best['best_f1']:.4f}\")\n",
        "print(f\"Stage 2 (ë²ˆì•„ì›ƒ 4ë¶„ë¥˜): Acc = {s2_best['best_acc']:.2f}%, F1 = {s2_best['best_f1']:.4f}\")\n",
        "\n",
        "print(\"\\nâš™ï¸ í•™ìŠµ ì„¤ì •\")\n",
        "print(\"-\"*70)\n",
        "print(f\"Stage 1: epochs={S1_CONFIG['epochs']}, lr={S1_CONFIG['lr']}, batch={S1_CONFIG['batch_size']}\")\n",
        "print(f\"         loss=CrossEntropy, label_smoothing={S1_CONFIG['label_smoothing']}\")\n",
        "print(f\"Stage 2: epochs={S2_CONFIG['epochs']}, lr={S2_CONFIG['lr']}, batch={S2_CONFIG['batch_size']}\")\n",
        "print(f\"         loss=FocalLoss(Î³=2.0), label_smoothing={S2_CONFIG['label_smoothing']}\")\n",
        "\n",
        "print(\"\\nğŸ†• v2 ì ìš© ê¸°ë²•\")\n",
        "print(\"-\"*70)\n",
        "print(\"1. Label Smoothing: ì˜¤ë²„í”¼íŒ… ë°©ì§€, ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\")\n",
        "print(\"2. Focal Loss (Stage 2): ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘, í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°\")\n",
        "print(\"3. Class Weight: ë°ì´í„° ë¶ˆê· í˜• ìë™ ë³´ì •\")\n",
        "print(\"4. Warmup + Cosine Annealing: ì•ˆì •ì ì¸ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§\")\n",
        "print(\"5. Early Stopping: patience=7, ê³¼ì í•© ë°©ì§€\")\n",
        "print(\"6. Gradient Clipping: max_norm=1.0, í•™ìŠµ ì•ˆì •í™”\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {"id": "summary"},
      "execution_count": null,
      "outputs": []
    }
  ]
}
