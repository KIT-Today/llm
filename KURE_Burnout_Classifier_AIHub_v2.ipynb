{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”¥ KURE ê¸°ë°˜ í•œêµ­í˜• ë²ˆì•„ì›ƒ ê°ì • ë¶„ë¥˜ ëª¨ë¸\n",
        "## AI Hub ë°ì´í„°ì…‹ í™œìš© ë²„ì „ (Fixed)\n",
        "\n",
        "**ìˆ˜ì •ì‚¬í•­**: inference_mode í…ì„œ ì—ëŸ¬ í•´ê²°\n",
        "\n",
        "**ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ (ë°•ìˆ˜ì • ì™¸, 2018)**:\n",
        "- 0: ì •ì„œì _ê³ ê°ˆ\n",
        "- 1: ì¢Œì ˆ_ì••ë°•\n",
        "- 2: ë¶€ì •ì _ëŒ€ì¸ê´€ê³„\n",
        "- 3: ìê¸°ë¹„í•˜"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. í™˜ê²½ ì„¤ì •"
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers transformers torch pandas scikit-learn tqdm openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from tqdm.auto import tqdm\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Google Drive ë§ˆìš´íŠ¸"
      ],
      "metadata": {
        "id": "data-upload-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë°ì´í„° ê²½ë¡œ ì„¤ì • (ë³¸ì¸ ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •!)\n",
        "DATA_PATH = \"/content/drive/MyDrive/Burnout/dataset\"\n",
        "\n",
        "if os.path.exists(DATA_PATH):\n",
        "    print(f\"âœ“ ë°ì´í„° ê²½ë¡œ í™•ì¸ë¨: {DATA_PATH}\")\n",
        "    print(f\"  í´ë” ë‚´ìš©: {os.listdir(DATA_PATH)}\")\n",
        "else:\n",
        "    print(f\"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {DATA_PATH}\")"
      ],
      "metadata": {
        "id": "drive-mount"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ & ê°ì • ë§¤í•‘"
      ],
      "metadata": {
        "id": "mapping-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BURNOUT_CATEGORIES = {\n",
        "    0: \"ì •ì„œì _ê³ ê°ˆ\",\n",
        "    1: \"ì¢Œì ˆ_ì••ë°•\",\n",
        "    2: \"ë¶€ì •ì _ëŒ€ì¸ê´€ê³„\",\n",
        "    3: \"ìê¸°ë¹„í•˜\"\n",
        "}\n",
        "\n",
        "EMOTION_TYPE_TO_BURNOUT = {\n",
        "    # ì •ì„œì  ê³ ê°ˆ (0)\n",
        "    \"E20\": 0, \"E22\": 0, \"E24\": 0, \"E26\": 0, \"E40\": 0, \"E53\": 0, \"E32\": 0,\n",
        "    # ì¢Œì ˆ/ì••ë°• (1)\n",
        "    \"E10\": 1, \"E15\": 1, \"E18\": 1, \"E19\": 1, \"E21\": 1, \"E47\": 1,\n",
        "    # ë¶€ì •ì  ëŒ€ì¸ê´€ê³„ (2)\n",
        "    \"E42\": 2, \"E49\": 2, \"E51\": 2,\n",
        "    # ìê¸°ë¹„í•˜ (3)\n",
        "    \"E16\": 3, \"E25\": 3, \"E30\": 3, \"E35\": 3, \"E37\": 3, \"E50\": 3, \"E54\": 3, \"E56\": 3, \"E57\": 3, \"E58\": 3,\n",
        "    # ê¸ì • (ì œì™¸)\n",
        "    \"E62\": -1, \"E64\": -1, \"E66\": -1, \"E67\": -1, \"E68\": -1, \"E69\": -1,\n",
        "}\n",
        "\n",
        "KEYWORD_TO_BURNOUT = {\n",
        "    \"ì§€ì¹¨\": 0, \"í”¼ê³¤\": 0, \"ë¬´ê¸°ë ¥\": 0, \"ìš°ìš¸\": 0, \"ìŠ¬í””\": 0, \"ê³µí—ˆ\": 0, \"ì™¸ë¡œ\": 0, \"í˜ë“¤\": 0, \"ì§€ì³\": 0, \"ìŠ¤íŠ¸ë ˆìŠ¤\": 0,\n",
        "    \"ë¶„ë…¸\": 1, \"í™”ë‚¨\": 1, \"í™”ê°€\": 1, \"ì§œì¦\": 1, \"ì–µìš¸\": 1, \"ë¶ˆë§Œ\": 1, \"ì‹¤ë§\": 1, \"ì„±ê°€\": 1, \"ê·€ì°®\": 1,\n",
        "    \"ë°°ì‹ \": 2, \"ê°ˆë“±\": 2, \"ë¬´ì‹œ\": 2, \"ì†Œì™¸\": 2, \"ì„œìš´\": 2, \"ë”°ëŒ\": 2, \"ì–´ìƒ‰\": 2,\n",
        "    \"ë¶ˆì•ˆ\": 3, \"ê±±ì •\": 3, \"ì´ˆì¡°\": 3, \"ë‘ë ¤\": 3, \"ë¶€ë„ëŸ¬\": 3, \"ì°½í”¼\": 3, \"ìì±…\": 3, \"ë‹¹í™©\": 3, \"ì—´ë“±\": 3,\n",
        "}\n",
        "\n",
        "print(\"âœ“ ë§¤í•‘ í…Œì´ë¸” ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "mapping"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. ë°ì´í„° ë¡œë”"
      ],
      "metadata": {
        "id": "loader-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_burnout_label(emotion_type: str, text: str = \"\") -> int:\n",
        "    if emotion_type in EMOTION_TYPE_TO_BURNOUT:\n",
        "        return EMOTION_TYPE_TO_BURNOUT[emotion_type]\n",
        "    for keyword, label in KEYWORD_TO_BURNOUT.items():\n",
        "        if keyword in text.lower():\n",
        "            return label\n",
        "    return -1\n",
        "\n",
        "\n",
        "def load_emotional_dialogue_json(json_path: str) -> pd.DataFrame:\n",
        "    print(f\"ğŸ“‚ Loading: {json_path}\")\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    records = []\n",
        "    for item in tqdm(data, desc=\"Parsing\"):\n",
        "        try:\n",
        "            emotion_info = item.get('profile', {}).get('emotion', {})\n",
        "            emotion_type = emotion_info.get('type', '')\n",
        "            content = item.get('talk', {}).get('content', {})\n",
        "\n",
        "            for key in ['HS01', 'HS02', 'HS03']:\n",
        "                text = content.get(key, '')\n",
        "                if text and len(text) >= 10:\n",
        "                    text = clean_text(text)\n",
        "                    burnout_label = get_burnout_label(emotion_type, text)\n",
        "                    records.append({\n",
        "                        'text': text,\n",
        "                        'emotion_type': emotion_type,\n",
        "                        'burnout_label': burnout_label\n",
        "                    })\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    print(f\"âœ“ Loaded {len(df)} samples\")\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"âœ“ ë°ì´í„° ë¡œë” ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "loader"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. ë°ì´í„° ë¡œë“œ"
      ],
      "metadata": {
        "id": "load-data-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotional_json_path = os.path.join(\n",
        "    DATA_PATH,\n",
        "    \"018.ê°ì„±ëŒ€í™”/Training_221115_add/ë¼ë²¨ë§ë°ì´í„°/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training/ê°ì„±ëŒ€í™”ë§ë­‰ì¹˜(ìµœì¢…ë°ì´í„°)_Training.json\"\n",
        ")\n",
        "\n",
        "if os.path.exists(emotional_json_path):\n",
        "    df_raw = load_emotional_dialogue_json(emotional_json_path)\n",
        "else:\n",
        "    print(f\"âŒ íŒŒì¼ ì—†ìŒ: {emotional_json_path}\")\n",
        "    df_raw = pd.DataFrame()"
      ],
      "metadata": {
        "id": "load-emotional"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í•„í„°ë§\n",
        "if len(df_raw) > 0:\n",
        "    df_filtered = df_raw[df_raw['burnout_label'] != -1].copy()\n",
        "    df_filtered = df_filtered.drop_duplicates(subset=['text'])\n",
        "\n",
        "    print(f\"\\nğŸ“Š ë°ì´í„°: {len(df_filtered)}ê°œ\")\n",
        "    print(\"\\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ ë¶„í¬:\")\n",
        "    for label, name in BURNOUT_CATEGORIES.items():\n",
        "        count = len(df_filtered[df_filtered['burnout_label'] == label])\n",
        "        print(f\"   {name}: {count}ê°œ ({100*count/len(df_filtered):.1f}%)\")\n",
        "else:\n",
        "    df_filtered = pd.DataFrame()"
      ],
      "metadata": {
        "id": "filter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Train/Val ë¶„í• "
      ],
      "metadata": {
        "id": "split-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if len(df_filtered) > 100:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df_filtered, test_size=0.1,\n",
        "        stratify=df_filtered['burnout_label'], random_state=42\n",
        "    )\n",
        "    print(f\"Train: {len(train_df)} / Val: {len(val_df)}\")\n",
        "else:\n",
        "    train_df, val_df = df_filtered, None\n",
        "    print(\"ë°ì´í„° ë¶€ì¡±, val ì—†ì´ ì§„í–‰\")"
      ],
      "metadata": {
        "id": "split"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. ğŸ”‘ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° (í•µì‹¬ ìˆ˜ì •!)\n",
        "\n",
        "**ë¬¸ì œ**: `SentenceTransformer.encode()`ê°€ `inference_mode`ë¡œ ì‹¤í–‰ë˜ì–´ backwardì™€ ì¶©ëŒ\n",
        "\n",
        "**í•´ê²°**: ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ì„œ ì €ì¥ â†’ í•™ìŠµ ì‹œì—ëŠ” ì„ë² ë”©ë§Œ ì‚¬ìš©"
      ],
      "metadata": {
        "id": "precompute-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KURE ëª¨ë¸ ë¡œë“œ\n",
        "print(\"ğŸ“¥ Loading KURE model...\")\n",
        "kure_model = SentenceTransformer(\"nlpai-lab/KURE-v1\", device=device)\n",
        "EMBEDDING_DIM = kure_model.get_sentence_embedding_dimension()\n",
        "print(f\"âœ“ KURE loaded, embedding dim: {EMBEDDING_DIM}\")"
      ],
      "metadata": {
        "id": "load-kure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° (í•™ìŠµ ì „ì— í•œ ë²ˆë§Œ!)\n",
        "print(\"\\nğŸ”„ Pre-computing embeddings...\")\n",
        "\n",
        "train_texts = train_df['text'].tolist()\n",
        "train_labels = train_df['burnout_label'].tolist()\n",
        "\n",
        "# ë°°ì¹˜ë¡œ ì„ë² ë”© ê³„ì‚°\n",
        "with torch.no_grad():\n",
        "    train_embeddings = kure_model.encode(\n",
        "        train_texts,\n",
        "        batch_size=64,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_tensor=True\n",
        "    )\n",
        "\n",
        "print(f\"âœ“ Train embeddings: {train_embeddings.shape}\")\n",
        "\n",
        "# Validation\n",
        "if val_df is not None and len(val_df) > 0:\n",
        "    val_texts = val_df['text'].tolist()\n",
        "    val_labels = val_df['burnout_label'].tolist()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_embeddings = kure_model.encode(\n",
        "            val_texts,\n",
        "            batch_size=64,\n",
        "            show_progress_bar=True,\n",
        "            convert_to_tensor=True\n",
        "        )\n",
        "    print(f\"âœ“ Val embeddings: {val_embeddings.shape}\")\n",
        "else:\n",
        "    val_embeddings, val_labels = None, None"
      ],
      "metadata": {
        "id": "precompute-embeddings"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. ë¶„ë¥˜ ëª¨ë¸ ì •ì˜ (ì„ë² ë”© ê¸°ë°˜)"
      ],
      "metadata": {
        "id": "model-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BurnoutClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    ë¶„ë¥˜ í—¤ë“œë§Œ í•™ìŠµ (ì„ë² ë”©ì€ ë¯¸ë¦¬ ê³„ì‚°ë¨)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=4, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        return self.classifier(embeddings)\n",
        "\n",
        "\n",
        "class EmbeddingDataset(Dataset):\n",
        "    \"\"\"ë¯¸ë¦¬ ê³„ì‚°ëœ ì„ë² ë”© ì‚¬ìš©\"\"\"\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "print(\"âœ“ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. í•™ìŠµ"
      ],
      "metadata": {
        "id": "training-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, train_embeddings, train_labels,\n",
        "                     val_embeddings=None, val_labels=None,\n",
        "                     epochs=10, batch_size=64, lr=1e-3, device='cuda'):\n",
        "\n",
        "    train_dataset = EmbeddingDataset(train_embeddings, train_labels)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_loader = None\n",
        "    if val_embeddings is not None:\n",
        "        val_dataset = EmbeddingDataset(val_embeddings, val_labels)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for emb, labels in pbar:\n",
        "            emb, labels = emb.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(emb)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, pred = torch.max(logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (pred == labels).sum().item()\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        history['train_loss'].append(total_loss / len(train_loader))\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        # Validation\n",
        "        val_acc = 0\n",
        "        if val_loader:\n",
        "            model.eval()\n",
        "            val_correct, val_total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for emb, labels in val_loader:\n",
        "                    emb, labels = emb.to(device), labels.to(device)\n",
        "                    logits = model(emb)\n",
        "                    _, pred = torch.max(logits, 1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += (pred == labels).sum().item()\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"  Loss: {history['train_loss'][-1]:.4f} | Train: {train_acc:.1f}% | Val: {val_acc:.1f}%\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "print(\"âœ“ Training í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ìƒì„± & í•™ìŠµ\n",
        "classifier = BurnoutClassifier(\n",
        "    input_dim=EMBEDDING_DIM,\n",
        "    hidden_dim=256,\n",
        "    num_classes=4,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"\\nğŸš€ í•™ìŠµ ì‹œì‘!\")\n",
        "print(f\"   Train: {len(train_labels)}ê°œ\")\n",
        "print(f\"   Val: {len(val_labels) if val_labels else 0}ê°œ\")\n",
        "\n",
        "classifier, history = train_classifier(\n",
        "    model=classifier,\n",
        "    train_embeddings=train_embeddings,\n",
        "    train_labels=train_labels,\n",
        "    val_embeddings=val_embeddings,\n",
        "    val_labels=val_labels,\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    lr=1e-3,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "run-training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. ëª¨ë¸ ì €ì¥"
      ],
      "metadata": {
        "id": "save-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì €ì¥\n",
        "save_path = \"/content/drive/MyDrive/Burnout/burnout_classifier.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': classifier.state_dict(),\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'num_classes': 4\n",
        "}, save_path)\n",
        "print(f\"âœ“ Saved: {save_path}\")"
      ],
      "metadata": {
        "id": "save-model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. ì¶”ë¡  í•¨ìˆ˜"
      ],
      "metadata": {
        "id": "inference-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(texts, kure_model, classifier, device='cuda'):\n",
        "    \"\"\"\n",
        "    í…ìŠ¤íŠ¸ â†’ ë²ˆì•„ì›ƒ ì¹´í…Œê³ ë¦¬ ì˜ˆì¸¡\n",
        "    \"\"\"\n",
        "    classifier.eval()\n",
        "\n",
        "    # 1. KUREë¡œ ì„ë² ë”©\n",
        "    with torch.no_grad():\n",
        "        embeddings = kure_model.encode(\n",
        "            texts,\n",
        "            convert_to_tensor=True,\n",
        "            show_progress_bar=False\n",
        "        ).to(device)\n",
        "\n",
        "        # 2. ë¶„ë¥˜\n",
        "        logits = classifier(embeddings)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        labels = torch.argmax(logits, dim=-1).cpu().tolist()\n",
        "\n",
        "    categories = [BURNOUT_CATEGORIES[l] for l in labels]\n",
        "    return labels, categories, probs\n",
        "\n",
        "\n",
        "print(\"âœ“ ì¶”ë¡  í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "inference-func"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. í…ŒìŠ¤íŠ¸"
      ],
      "metadata": {
        "id": "test-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"ì˜¤ëŠ˜ë„ ì•¼ê·¼ì´ë‹¤. ë„ˆë¬´ ì§€ì¹˜ê³  í˜ë“¤ë‹¤.\",\n",
        "    \"ìƒì‚¬ê°€ ë˜ í™”ë¥¼ ëƒˆë‹¤. ì–µìš¸í•˜ê³  ë¶„í•˜ë‹¤.\",\n",
        "    \"íŒ€ì›ë“¤ì´ ë‚˜ë¥¼ ë¬´ì‹œí•˜ëŠ” ê²ƒ ê°™ë‹¤.\",\n",
        "    \"ë‚˜ëŠ” ì™œ ì´ê²ƒë°–ì— ëª»í•˜ëŠ” ê±¸ê¹Œ. ë¶ˆì•ˆí•˜ë‹¤.\",\n",
        "    \"ì¼ì€ ì™œ í•´ë„ í•´ë„ ëì´ ì—†ì„ê¹Œ? í™”ê°€ ë‚œë‹¤.\",\n",
        "]\n",
        "\n",
        "labels, categories, probs = predict(test_texts, kure_model, classifier, device)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ”® ì˜ˆì¸¡ ê²°ê³¼\")\n",
        "print(\"=\"*60)\n",
        "for text, cat, prob in zip(test_texts, categories, probs):\n",
        "    print(f\"\\nì…ë ¥: {text}\")\n",
        "    print(f\"ì˜ˆì¸¡: {cat}\")\n",
        "    prob_str = {k: f\"{v:.1%}\" for k, v in zip(BURNOUT_CATEGORIES.values(), prob.cpu().numpy())}\n",
        "    print(f\"í™•ë¥ : {prob_str}\")"
      ],
      "metadata": {
        "id": "test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸"
      ],
      "metadata": {
        "id": "custom-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_text = \"ì›”ìš”ì¼ ì•„ì¹¨ë¶€í„° ì•¼ê·¼ ì˜ˆê³ ë¥¼ ë°›ì•˜ë‹¤. ë²Œì¨ ì§€ì¹œë‹¤.\"  # @param {type:\"string\"}\n",
        "\n",
        "labels, categories, probs = predict([my_text], kure_model, classifier, device)\n",
        "\n",
        "print(f\"\\nì…ë ¥: {my_text}\")\n",
        "print(f\"\\nğŸ·ï¸ ì˜ˆì¸¡: {categories[0]}\")\n",
        "print(f\"\\nğŸ“Š í™•ë¥ :\")\n",
        "for name, p in zip(BURNOUT_CATEGORIES.values(), probs[0].cpu().numpy()):\n",
        "    bar = \"â–ˆ\" * int(p * 20)\n",
        "    print(f\"  {name}: {bar} {p:.1%}\")"
      ],
      "metadata": {
        "id": "custom-test"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
      ],
      "metadata": {
        "id": "viz-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], label='Train Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training Loss')\n",
        "axes[0].legend()\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history['train_acc'], label='Train Acc')\n",
        "if history['val_acc']:\n",
        "    axes[1].plot(history['val_acc'], label='Val Acc')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Accuracy')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nìµœì¢… ì„±ëŠ¥:\")\n",
        "print(f\"  Train Acc: {history['train_acc'][-1]:.1f}%\")\n",
        "if history['val_acc']:\n",
        "    print(f\"  Val Acc: {history['val_acc'][-1]:.1f}%\")"
      ],
      "metadata": {
        "id": "viz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
