"""
ğŸ”¥ ë²ˆì•„ì›ƒ 2ë‹¨ê³„ ë¶„ë¥˜ + í”¼ë“œë°± ìƒì„± í…ŒìŠ¤íŠ¸
==========================================

ì‚¬ìš©ë²•:
    python test_burnout_full.py                    # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    python test_burnout_full.py --text "í˜ë“¤ë‹¤"    # ë‹¨ì¼ í…ìŠ¤íŠ¸
    python test_burnout_full.py -i                 # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ

í•„ìš” íŒŒì¼:
    - stage1_model.pt
    - stage2_model.pt

í•„ìš” íŒ¨í‚¤ì§€:
    pip install torch sentence-transformers transformers accelerate
"""

import argparse
import json
import re
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

# ============================================
# ì„¤ì •
# ============================================

STAGE1_CATEGORIES = {0: "ê¸ì •", 1: "ë¶€ì •"}
STAGE2_CATEGORIES = {0: "ì •ì„œì _ê³ ê°ˆ", 1: "ì¢Œì ˆ_ì••ë°•", 2: "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„", 3: "ìê¸°ë¹„í•˜"}

BURNOUT_KEYWORDS = {
    "ê¸ì •": {"keywords": ["ì¢‹ë‹¤", "ì¢‹ì•„", "í–‰ë³µ", "ê¸°ì˜", "ë¿Œë“¯", "ë§Œì¡±", "ê°ì‚¬", "ê³ ë§™", "ë‹¤í–‰", "í™€ê°€ë¶„", "ìƒì¾Œ", "íë§", "í¸ì•ˆ", "ì—¬ìœ ", "ì„±ê³µ", "ë‹¬ì„±", "ì™„ë£Œ", "ëë‚¬", "ì¹­ì°¬", "ì¸ì •", "ë³´ëŒ", "ì¦ê²", "ì‹ ë‚˜", "ì„¤ë ˆ", "ê¸°ëŒ€", "í¬ë§", "ì›ƒ"]},
    "ë¶€ì •": {"keywords": ["í˜ë“¤", "ì§€ì¹˜", "í”¼ê³¤", "ì‹«", "ì§œì¦", "í™”ë‚˜", "ì–µìš¸", "ìŠ¬í”„", "ìš°ìš¸", "ë¶ˆì•ˆ", "ê±±ì •", "ë¬´ì„­", "ë‘ë µ", "ì™¸ë¡­", "ì„œìš´", "ì‹¤ë§", "í›„íšŒ", "ë¯¸ì•ˆ"]},
    "ì •ì„œì _ê³ ê°ˆ": {"keywords": ["ì§€ì¹˜", "í”¼ê³¤", "í˜ë“¤", "ë¬´ê¸°ë ¥", "íƒˆì§„", "ë…¹ì´ˆ", "ë°©ì „", "ì§€ì³", "ì˜ìš•", "ì—ë„ˆì§€", "ê¸°ìš´", "ë¬´ê±°", "ê³µí—ˆ", "í……", "ë¹„ì–´", "ë©”ë§ˆë¥´", "ë²ˆì•„ì›ƒ", "ìš°ìš¸", "ìŠ¬í”„", "ëˆˆë¬¼", "í—ˆë¬´", "ë¬´ì˜ë¯¸", "ì‹«ì–´", "ê·€ì°®"]},
    "ì¢Œì ˆ_ì••ë°•": {"keywords": ["í™”ë‚˜", "í™”ê°€", "ì§œì¦", "ì—´ë°›", "ë¹¡ì¹˜", "ë¶„ë…¸", "ì–µìš¸", "ë¶ˆê³µí‰", "ì••ë°•", "ìŠ¤íŠ¸ë ˆìŠ¤", "ë§ˆê°", "ë‹µë‹µ", "ë¯¸ì¹˜", "í­ë°œ", "í•œê³„", "ëª»ì°¸", "ì™œ", "ë„ëŒ€ì²´", "ì§“ëˆŒ", "ê°ë‹¹", "ë¶€ë‹´", "ì‹¤ì ", "ì•ˆë˜", "ì•ˆí’€"]},
    "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„": {"keywords": ["ë¬´ì‹œ", "ì†Œì™¸", "ë”°ëŒ", "ì™•ë”°", "ë°°ì‹ ", "ë’·ë‹´", "í—˜ë‹´", "ê°ˆë“±", "ì‹¸ìš°", "ë‹¤íˆ¬", "í‹€ì–´", "ì†Œë¬¸", "ì˜¤í•´", "ë¯¿ì—ˆ", "ì‹¤ë§", "ì„œìš´", "í˜¼ì", "ì™¸ë¡œ", "í¸ì—†", "ê±°ì ˆ", "ë¹¼ê³ ", "ì•ˆë¼", "ì •ì¹˜", "ëˆˆì¹˜"]},
    "ìê¸°ë¹„í•˜": {"keywords": ["ëª»í•˜", "ëª»ë‚œ", "ë¶€ì¡±", "ë¬´ëŠ¥", "í•œì‹¬", "ìê²©", "ë¶ˆì•ˆ", "ê±±ì •", "ìì±…", "ì£„ì±…", "ì˜ëª»", "ë‚´íƒ“", "ë¯¸ì•ˆ", "í›„íšŒ", "ì—´ë“±", "ë¹„êµ", "ì™œë‚˜ë§Œ", "ìì‹ ì—†", "ë‘ë µ", "ë¬´ì„­", "ì‹¤íŒ¨", "ë§", "ê°€ì¹˜ì—†", "ì“¸ëª¨ì—†"]},
}

# í”¼ë“œë°± í…œí”Œë¦¿ (LLM ì—†ì´ ì‚¬ìš©)
FEEDBACK_TEMPLATES = {
    "ê¸ì •": [
        "ì˜¤ëŠ˜ í•˜ë£¨ë„ ìˆ˜ê³ í•˜ì…¨ì–´ìš”! ì¢‹ì€ ì—ë„ˆì§€ê°€ ëŠê»´ì§€ë„¤ìš”. ğŸ˜Š",
        "ê¸ì •ì ì¸ í•˜ë£¨ë¥¼ ë³´ë‚´ê³  ê³„ì‹œë„¤ìš”. ê·¸ ê¸°ìš´ ê³„ì† ì´ì–´ê°€ì„¸ìš”!",
    ],
    "ì •ì„œì _ê³ ê°ˆ": [
        "ë§ì´ ì§€ì¹˜ì…¨ë„¤ìš”. ì˜¤ëŠ˜ì€ ì¼ì° ì‰¬ì–´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
        "ì—ë„ˆì§€ê°€ ë°”ë‹¥ë‚œ ëŠë‚Œì´ì‹œì£ . ì ê¹ ìˆ¨ ê³ ë¥´ëŠ” ì‹œê°„ì´ í•„ìš”í•´ìš”.",
        "ì§€ì¹œ ë§ˆìŒ, ì¶©ë¶„íˆ ì´í•´í•´ìš”. ì˜¤ëŠ˜ í•˜ë£¨ ì •ë§ ìˆ˜ê³  ë§ìœ¼ì…¨ì–´ìš”.",
    ],
    "ì¢Œì ˆ_ì••ë°•": [
        "ì–µìš¸í•˜ê³  ë‹µë‹µí•œ ë§ˆìŒì´ ëŠê»´ì ¸ìš”. ê·¸ ê°ì •ì€ ë‹¹ì—°í•œ ê±°ì˜ˆìš”.",
        "í™”ê°€ ë‚˜ëŠ” ê±´ ìì—°ìŠ¤ëŸ¬ìš´ ê°ì •ì´ì—ìš”. ì ì‹œ ê¹Šê²Œ ìˆ¨ì„ ì‰¬ì–´ë³´ì„¸ìš”.",
        "ì••ë°•ê° ì†ì—ì„œë„ ë²„í‹°ê³  ê³„ì‹œë„¤ìš”. ëŒ€ë‹¨í•˜ì„¸ìš”. ì ì‹œ ì‰¬ì–´ê°€ë„ ê´œì°®ì•„ìš”.",
    ],
    "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„": [
        "ê´€ê³„ì—ì„œ ìƒì²˜ë°›ìœ¼ì…¨êµ°ìš”. ê·¸ ë§ˆìŒì´ ì–¼ë§ˆë‚˜ í˜ë“œì‹¤ì§€ ëŠê»´ì ¸ìš”.",
        "ì‚¬ëŒ ì‚¬ì´ì—ì„œ ì˜¤ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ëŠ” ì •ë§ í˜ë“¤ì£ . í˜¼ìê°€ ì•„ë‹ˆì—ìš”.",
        "ì„œìš´í•œ ë§ˆìŒ, ì¶©ë¶„íˆ ì´í•´í•´ìš”. ë‹¹ì‹  ì˜ëª»ì´ ì•„ë‹ˆì—ìš”.",
    ],
    "ìê¸°ë¹„í•˜": [
        "ìì‹ ì„ ë„ˆë¬´ íƒ“í•˜ì§€ ë§ˆì„¸ìš”. ë‹¹ì‹ ì€ ì¶©ë¶„íˆ ì˜í•˜ê³  ìˆì–´ìš”.",
        "ë¶ˆì•ˆí•œ ë§ˆìŒì´ ë“œì‹œëŠ”êµ°ìš”. ê·¸ë˜ë„ ë‹¹ì‹ ì€ ê°€ì¹˜ ìˆëŠ” ì‚¬ëŒì´ì—ìš”.",
        "ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ê´œì°®ì•„ìš”. ì§€ê¸ˆ ì´ ìˆœê°„ë„ ì˜ í•´ë‚´ê³  ìˆì–´ìš”.",
    ],
}


# ============================================
# ëª¨ë¸ ì •ì˜
# ============================================

class BurnoutClassifier(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=256, num_classes=2, dropout=0.5):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, num_classes)
        )

    def forward(self, x):
        return self.classifier(x)


# ============================================
# Explainer
# ============================================

class BurnoutExplainer:
    def __init__(self, kure_model, stage1_model, stage2_model, device="cpu"):
        self.kure = kure_model
        self.stage1 = stage1_model
        self.stage2 = stage2_model
        self.device = device

    def _tokenize(self, text):
        tokens = re.findall(r"[ê°€-í£]+", text)
        return [t for t in tokens if len(t) >= 2]

    def _predict_stage1(self, text):
        self.stage1.eval()
        with torch.no_grad():
            emb = self.kure.encode(text, convert_to_tensor=True).unsqueeze(0).to(self.device)
            logits = self.stage1(emb)
            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
            pred = int(np.argmax(probs))
        return pred, probs

    def _predict_stage2(self, text):
        self.stage2.eval()
        with torch.no_grad():
            emb = self.kure.encode(text, convert_to_tensor=True).unsqueeze(0).to(self.device)
            logits = self.stage2(emb)
            probs = F.softmax(logits, dim=-1)[0].cpu().numpy()
            pred = int(np.argmax(probs))
        return pred, probs

    def attention_analysis(self, text, stage="stage1", top_k=5):
        tokens = self._tokenize(text)
        if not tokens:
            return []
        if stage == "stage1":
            base_pred, base_probs = self._predict_stage1(text)
        else:
            base_pred, base_probs = self._predict_stage2(text)
        base_conf = base_probs[base_pred]
        importance = {}
        for token in tokens:
            modified = text.replace(token, "", 1)
            if modified.strip():
                if stage == "stage1":
                    _, new_probs = self._predict_stage1(modified)
                else:
                    _, new_probs = self._predict_stage2(modified)
                change = base_conf - new_probs[base_pred]
                importance[token] = max(0, change)
        total = sum(importance.values()) + 1e-10
        importance = {k: v/total for k, v in importance.items()}
        sorted_tokens = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:top_k]
        return [{"token": t, "score": round(s, 4)} for t, s in sorted_tokens]

    def shap_analysis(self, text, stage="stage1", top_k=5):
        tokens = self._tokenize(text)
        if not tokens:
            return []
        if stage == "stage1":
            base_pred, base_probs = self._predict_stage1(text)
        else:
            base_pred, base_probs = self._predict_stage2(text)
        contributions = {}
        for token in tokens:
            modified = text.replace(token, "", 1)
            if modified.strip():
                if stage == "stage1":
                    _, new_probs = self._predict_stage1(modified)
                else:
                    _, new_probs = self._predict_stage2(modified)
                contrib = base_probs[base_pred] - new_probs[base_pred]
                contributions[token] = contrib
        sorted_tokens = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)[:top_k]
        return [{"token": t, "contribution": round(c, 4), "direction": "positive" if c > 0 else "negative"} for t, c in sorted_tokens]

    def keyword_analysis(self, text, categories=None):
        if categories is None:
            categories = list(BURNOUT_KEYWORDS.keys())
        text_lower = text.lower()
        results = {}
        for cat in categories:
            if cat not in BURNOUT_KEYWORDS:
                continue
            keywords = BURNOUT_KEYWORDS[cat]["keywords"]
            matched = [kw for kw in keywords if kw in text_lower]
            match_rate = len(matched) / len(keywords) if keywords else 0
            results[cat] = {"matched_keywords": matched, "match_count": len(matched), "match_rate": round(match_rate, 4)}
        return results

    def explain(self, text, top_k=5):
        s1_pred, s1_probs = self._predict_stage1(text)
        result = {
            "text": text,
            "stage1": {
                "prediction": {"label": s1_pred, "category": STAGE1_CATEGORIES[s1_pred], "confidence": float(s1_probs[s1_pred])},
                "probabilities": {STAGE1_CATEGORIES[i]: float(p) for i, p in enumerate(s1_probs)},
                "attention": self.attention_analysis(text, "stage1", top_k),
                "shap": self.shap_analysis(text, "stage1", top_k),
                "keywords": self.keyword_analysis(text, ["ê¸ì •", "ë¶€ì •"])
            }
        }
        if s1_pred == 1:
            s2_pred, s2_probs = self._predict_stage2(text)
            result["stage2"] = {
                "prediction": {"label": s2_pred, "category": STAGE2_CATEGORIES[s2_pred], "confidence": float(s2_probs[s2_pred])},
                "probabilities": {STAGE2_CATEGORIES[i]: float(p) for i, p in enumerate(s2_probs)},
                "attention": self.attention_analysis(text, "stage2", top_k),
                "shap": self.shap_analysis(text, "stage2", top_k),
                "keywords": self.keyword_analysis(text, list(STAGE2_CATEGORIES.values()))
            }
        return result


# ============================================
# í”¼ë“œë°± ìƒì„±
# ============================================

def generate_feedback_template(category, keywords=None):
    """í…œí”Œë¦¿ ê¸°ë°˜ í”¼ë“œë°± ìƒì„±"""
    import random
    templates = FEEDBACK_TEMPLATES.get(category, FEEDBACK_TEMPLATES["ì •ì„œì _ê³ ê°ˆ"])
    return random.choice(templates)


def generate_feedback_llm(category, user_text, keywords, generator, tokenizer):
    """LLM ê¸°ë°˜ í”¼ë“œë°± ìƒì„±"""
    CATEGORY_CONTEXT = {
        "ì •ì„œì _ê³ ê°ˆ": "ì§€ì¹˜ê³  ë¬´ê¸°ë ¥í•œ ìƒíƒœì…ë‹ˆë‹¤.",
        "ì¢Œì ˆ_ì••ë°•": "ì–µìš¸í•˜ê³  í™”ê°€ ë‚˜ëŠ” ìƒíƒœì…ë‹ˆë‹¤.",
        "ë¶€ì •ì _ëŒ€ì¸ê´€ê³„": "ëŒ€ì¸ê´€ê³„ì—ì„œ ìƒì²˜ë°›ì€ ìƒíƒœì…ë‹ˆë‹¤.",
        "ìê¸°ë¹„í•˜": "ìì‹ ì„ íƒ“í•˜ê³  ë¶ˆì•ˆí•´í•˜ëŠ” ìƒíƒœì…ë‹ˆë‹¤."
    }
    
    prompt = f"""### ëª…ë ¹ì–´:
ë‹¹ì‹ ì€ ì§ì¥ì¸ì˜ ë²ˆì•„ì›ƒì„ ì¼€ì–´í•˜ëŠ” ë”°ëœ»í•œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœë¥¼ ë³´ê³ , ê³µê°í•˜ë©° ìœ„ë¡œí•˜ëŠ” 2-3ë¬¸ì¥ì„ ì‘ì„±í•˜ì„¸ìš”.

ê·œì¹™:
- ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ í†¤
- ì‚¬ìš©ìì˜ ê°ì •ì„ ì¸ì •í•˜ê³  ê³µê°
- ì‘ì€ í–‰ë™ ì œì•ˆ (ê°•ìš”í•˜ì§€ ì•Šê¸°)

### ì…ë ¥:
ê°ì • ìƒíƒœ: {category} - {CATEGORY_CONTEXT.get(category, '')}
ì‚¬ìš©ì ì¼ê¸°: "{user_text}"
ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords) if keywords else 'ì—†ìŒ'}

### ì‘ë‹µ:
"""
    
    result = generator(
        prompt,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.eos_token_id
    )
    
    generated = result[0]['generated_text']
    response = generated.split("### ì‘ë‹µ:")[-1].strip()
    response = response.split("\n\n")[0].strip()
    
    return response


# ============================================
# ì „ì²´ íŒŒì´í”„ë¼ì¸
# ============================================

class BurnoutPipeline:
    def __init__(self, model_dir=".", device=None, use_llm=False):
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device
        
        print(f"ğŸ–¥ï¸ Device: {self.device}")
        
        # KURE ë¡œë“œ
        print("ğŸ“¥ Loading KURE...")
        from sentence_transformers import SentenceTransformer
        self.kure = SentenceTransformer("nlpai-lab/KURE-v1", device=self.device)
        
        # Stage 1 ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ Loading Stage 1 model...")
        s1_path = f"{model_dir}/stage1_model.pt"
        s1_ckpt = torch.load(s1_path, map_location=self.device, weights_only=False)
        self.stage1 = BurnoutClassifier(
            input_dim=s1_ckpt.get('embedding_dim', 1024),
            hidden_dim=s1_ckpt.get('hidden_dim', 256),
            num_classes=2
        ).to(self.device)
        self.stage1.load_state_dict(s1_ckpt['model_state_dict'])
        self.stage1.eval()
        
        # Stage 2 ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ Loading Stage 2 model...")
        s2_path = f"{model_dir}/stage2_model.pt"
        s2_ckpt = torch.load(s2_path, map_location=self.device, weights_only=False)
        self.stage2 = BurnoutClassifier(
            input_dim=s2_ckpt.get('embedding_dim', 1024),
            hidden_dim=s2_ckpt.get('hidden_dim', 256),
            num_classes=4
        ).to(self.device)
        self.stage2.load_state_dict(s2_ckpt['model_state_dict'])
        self.stage2.eval()
        
        # Explainer ìƒì„±
        self.explainer = BurnoutExplainer(self.kure, self.stage1, self.stage2, self.device)
        
        # LLM ë¡œë“œ (ì„ íƒ)
        self.use_llm = use_llm
        self.generator = None
        self.tokenizer = None
        
        if use_llm:
            print("ğŸ“¥ Loading KoAlpaca...")
            from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
            MODEL_NAME = "beomi/KoAlpaca-Polyglot-5.8B"
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                device_map="auto",
                torch_dtype=torch.float16
            )
            self.generator = pipeline(
                "text-generation",
                model=model,
                tokenizer=self.tokenizer,
                device_map="auto"
            )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    def analyze(self, text):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        # ë¶„ë¥˜ + íŒë‹¨ ê·¼ê±°
        explanation = self.explainer.explain(text)
        
        s1_result = explanation['stage1']['prediction']
        
        result = {
            'text': text,
            'stage1': s1_result,
            'is_positive': s1_result['category'] == 'ê¸ì •',
            'explanation': explanation
        }
        
        # ë¶€ì •ì´ë©´ Stage 2 + í”¼ë“œë°±
        if not result['is_positive']:
            s2_result = explanation['stage2']['prediction']
            result['stage2'] = s2_result
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = [item['token'] for item in explanation['stage2']['attention'][:3]]
            result['keywords'] = keywords
            
            # í”¼ë“œë°± ìƒì„±
            if self.use_llm and self.generator:
                feedback = generate_feedback_llm(
                    s2_result['category'], text, keywords,
                    self.generator, self.tokenizer
                )
            else:
                feedback = generate_feedback_template(s2_result['category'], keywords)
            result['feedback'] = feedback
        else:
            result['feedback'] = generate_feedback_template('ê¸ì •')
        
        return result
    
    def to_api_response(self, result):
        """APIìš© JSON ë³€í™˜"""
        response = {
            "input_text": result['text'],
            "is_positive": result['is_positive'],
            "stage1": {
                "category": result['stage1']['category'],
                "confidence": round(result['stage1']['confidence'], 4)
            },
            "feedback": result['feedback']
        }
        
        if not result['is_positive']:
            response["stage2"] = {
                "category": result['stage2']['category'],
                "confidence": round(result['stage2']['confidence'], 4)
            }
            response["keywords"] = result['keywords']
            
            exp = result['explanation']['stage2']
            response["explanation"] = {
                "attention": exp['attention'][:3],
                "shap": exp['shap'][:3],
                "matched_keywords": exp['keywords'][result['stage2']['category']]['matched_keywords']
            }
        
        return response


# ============================================
# ì¶œë ¥ í•¨ìˆ˜
# ============================================

def print_result(result):
    """ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "="*70)
    print(f"ğŸ“ ì…ë ¥: {result['text']}")
    print("="*70)
    
    s1 = result['stage1']
    print(f"\n[Stage 1] {s1['category']} ({s1['confidence']:.1%})")
    
    if result['is_positive']:
        print("\nâœ… ìƒíƒœ: ê¸ì •")
    else:
        s2 = result['stage2']
        print(f"[Stage 2] {s2['category']} ({s2['confidence']:.1%})")
        print(f"\nâš ï¸ ìƒíƒœ: {s2['category']} (ë²ˆì•„ì›ƒ ì§•í›„)")
        print(f"ğŸ”‘ í‚¤ì›Œë“œ: {', '.join(result['keywords'])}")
        
        # íŒë‹¨ ê·¼ê±°
        exp = result['explanation']['stage2']
        print("\nğŸ“Š íŒë‹¨ ê·¼ê±°:")
        print("  [Attention]", end=" ")
        for item in exp['attention'][:3]:
            print(f"{item['token']}({item['score']:.0%})", end=" ")
        print()
        print("  [SHAP]", end=" ")
        for item in exp['shap'][:3]:
            sign = "+" if item['direction'] == 'positive' else "-"
            print(f"{item['token']}({sign}{abs(item['contribution']):.3f})", end=" ")
        print()
    
    print(f"\nğŸ’¬ í”¼ë“œë°±:\n{result['feedback']}")
    print("="*70)


# ============================================
# ë©”ì¸
# ============================================

def main():
    parser = argparse.ArgumentParser(description="ë²ˆì•„ì›ƒ 2ë‹¨ê³„ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸")
    parser.add_argument("--text", "-t", type=str, help="ë¶„ì„í•  í…ìŠ¤íŠ¸")
    parser.add_argument("--interactive", "-i", action="store_true", help="ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ")
    parser.add_argument("--model-dir", "-m", type=str, default=".", help="ëª¨ë¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--use-llm", action="store_true", help="LLM í”¼ë“œë°± ì‚¬ìš© (KoAlpaca)")
    parser.add_argument("--json", "-j", action="store_true", help="JSON ì¶œë ¥")
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = BurnoutPipeline(model_dir=args.model_dir, use_llm=args.use_llm)
    
    if args.text:
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¶„ì„
        result = pipeline.analyze(args.text)
        if args.json:
            print(json.dumps(pipeline.to_api_response(result), ensure_ascii=False, indent=2))
        else:
            print_result(result)
    
    elif args.interactive:
        # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
        print("\nğŸ® ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ (ì¢…ë£Œ: q)")
        print("-"*50)
        while True:
            text = input("\nì…ë ¥> ").strip()
            if text.lower() in ['q', 'quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            if not text:
                continue
            
            result = pipeline.analyze(text)
            if args.json:
                print(json.dumps(pipeline.to_api_response(result), ensure_ascii=False, indent=2))
            else:
                print_result(result)
    
    else:
        # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
        test_texts = [
            "ì˜¤ëŠ˜ ì •ë§ ìµœì•…ì´ë‹¤.",
        ]
        
        print("\nğŸ§ª ìƒ˜í”Œ í…ŒìŠ¤íŠ¸")
        for text in test_texts:
            result = pipeline.analyze(text)
            print_result(result)
        
        print("\nğŸ“¤ API Response ì˜ˆì‹œ:")
        result = pipeline.analyze("ìƒì‚¬ê°€ ë˜ í™”ë¥¼ ëƒˆë‹¤. ì–µìš¸í•˜ê³  ë¶„í•˜ë‹¤.")
        print(json.dumps(pipeline.to_api_response(result), ensure_ascii=False, indent=2))


if __name__ == "__main__":
    main()
